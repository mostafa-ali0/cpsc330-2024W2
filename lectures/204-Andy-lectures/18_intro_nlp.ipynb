{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a56f0ca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(\"..\"), \"code\"))\n",
    "\n",
    "import IPython\n",
    "import matplotlib.pyplot as pp\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "from comat import CooccurrenceMatrix\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from preprocessing import MyPreprocessor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89261e47",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/andrew/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You might need to run this first if the imports error out\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beb4baf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 18: Introduction to natural language processing \n",
    "\n",
    "UBC 2025\n",
    "\n",
    "Instructor: Andrew Roth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b543eac5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning objectives\n",
    "\n",
    "- Broadly explain what is natural language processing (NLP). \n",
    "- Name some common NLP applications. \n",
    "- Explain the general idea of a vector space model.\n",
    "- Explain the difference between different word representations: term-term co-occurrence matrix representation and Word2Vec representation.\n",
    "- Describe the reasons and benefits of using pre-trained embeddings. \n",
    "- Load and use pre-trained word embeddings to find word similarities and analogies. \n",
    "- Demonstrate biases in embeddings and learn to watch out for such biases in pre-trained embeddings.\n",
    "- Use word embeddings in text classification and document clustering using `spaCy`.\n",
    "- Explain the general idea of topic modeling. \n",
    "- Describe the input and output of topic modeling. \n",
    "- Carry out basic text preprocessing using `spaCy`.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a58057",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Natural Language Processing (NLP)?\n",
    "\n",
    "- What should a search engine return when asked the following question? \n",
    "\n",
    "![](../img/lexical_ambiguity.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/lexical_ambiguity.png\" width=\"1000\" height=\"1000\"> -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81795e3f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What is Natural Language Processing (NLP)?\n",
    "#### How often do you search everyday? \n",
    "\n",
    "![](../img/Google_search.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/Google_search.png\" width=\"900\" height=\"900\"> -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ae2164",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What is Natural Language Processing (NLP)?\n",
    "\n",
    "![](../img/WhatisNLP.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/WhatisNLP.png\" width=\"800\" height=\"800\"> -->\n",
    "<!-- </center> -->  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae09dba",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Everyday NLP applications\n",
    "\n",
    "![](../img/annotation-image.png)\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/annotation-image.png\" height=\"1200\" width=\"1200\"> -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72d9284",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### NLP in news \n",
    "\n",
    "Often you'll see NLP in news. Some examples: \n",
    "- [How suicide prevention is getting a boost from artificial intelligence](https://abcnews.go.com/GMA/Wellness/suicide-prevention-boost-artificial-intelligence-exclusive/story?id=76541481)\n",
    "- [Meet GPT-3. It Has Learned to Code (and Blog and Argue).](https://www.nytimes.com/2020/11/24/science/artificial-intelligence-ai-gpt3.html)\n",
    "- [How Do You Know a Human Wrote This?](https://www.nytimes.com/2020/07/29/opinion/gpt-3-ai-automation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7e1518",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why is NLP hard?\n",
    "\n",
    "- Language is complex and subtle. \n",
    "- Language is ambiguous at different levels. \n",
    "- Language understanding involves common-sense knowledge and real-world reasoning.\n",
    "- All the problems related to representation and reasoning in artificial intelligence arise in this domain. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74edcf4b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: Lexical ambiguity\n",
    "\n",
    "<br><br>\n",
    "\n",
    "![](../img/lexical_ambiguity.png)\n",
    "\n",
    "<!-- <img src=\"img/lexical_ambiguity.png\" width=\"1000\" height=\"1000\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9147d6b8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: Referential ambiguity\n",
    "<br><br>\n",
    "\n",
    "<!-- <img src=\"img/referential_ambiguity.png\" width=\"1000\" height=\"1000\"> -->\n",
    "\n",
    "![](../img/referential_ambiguity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac18c16e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### [Ambiguous news headlines](http://www.fun-with-words.com/ambiguous_headlines.html)\n",
    "\n",
    "<blockquote>\n",
    "PROSTITUTES APPEAL TO POPE\n",
    "</blockquote>    \n",
    "\n",
    "- **appeal to** means make a serious or urgent request or be attractive or interesting?\n",
    "\n",
    "<blockquote>\n",
    "KICKING BABY CONSIDERED TO BE HEALTHY    \n",
    "</blockquote> \n",
    "\n",
    "- **kicking** is used as an adjective or a verb?\n",
    "\n",
    "<blockquote>\n",
    "MILK DRINKERS ARE TURNING TO POWDER\n",
    "</blockquote>\n",
    "\n",
    "- **turning** means becoming or take up?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b94a2dc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Overall goal\n",
    "\n",
    "- Give you a quick introduction to you of this important field in artificial intelligence which extensively used machine learning.   \n",
    "\n",
    "![](../img/NLP_in_industry.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/NLP_in_industry.png\" width=\"900\" height=\"800\"> -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9452f2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Today's plan\n",
    "\n",
    "- Word embeddings\n",
    "- Topic modeling \n",
    "- Basic text preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c01cab1-9cd0-450e-afff-36161cc2cee9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Motivation and context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930286d5-a4d4-418a-8aa8-24500d5e2d52",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "- Do large language models, such as ChatGPT, \"understand\" your questions to some extent and provide useful responses?\n",
    "- What is required for a machine to \"understand\" language?  \n",
    "- So far we have been talking about sentence or document representations. \n",
    "- This week, we'll go one step back and talk about word representations. \n",
    "- Why? Because word is a basic semantic unit of text and in order to capture meaning of text it is useful to capture word meaning (e.g., in terms of relationships between words).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2acdc84-2a80-4270-8143-e92cf5d68630",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Activity: Context and word meaning \n",
    "\n",
    "- Pair up with the person next to you and try to guess the meanings of two made-up words: **flibbertigibbet** and **groak**.\n",
    "\n",
    "\n",
    "> 1. The plot twist was totally unexpected, making it a **flibbertigibbet** experience.\n",
    "> 2. Despite its **groak** special effects, the storyline captivated my attention till the end.\n",
    "> 3. I found the character development rather **groak**, failing to evoke empathy.\n",
    "> 4. The cinematography is **flibbertigibbet**, showcasing breathtaking landscapes.\n",
    "> 5. A **groak** narrative that could have been saved with better direction.\n",
    "> 6. This movie offers a **flibbertigibbet** blend of humour and action, a must-watch.\n",
    "> 7. Sadly, the movie's potential was overshadowed by its **groak** pacing.\n",
    "> 8. The soundtrack complemented the film's theme perfectly, adding to its **flibbertigibbet** charm.\n",
    "> 9. It's rare to see such a **flibbertigibbet** performance by the lead actor.\n",
    "> 10. Despite high expectations, the film turned out to be quite **groak**.\n",
    "> 11. **Flibbertigibbet** dialogues and a gripping plot make this movie stand out.\n",
    "> 12. The film's **groak** screenplay left much to be desired.\n",
    "\n",
    "Attributions: Thanks to ChatGPT! \n",
    "\n",
    "- How did you infer the meaning of the words **flibbertigibbet** and **groak**?\n",
    "- Which specific words or phrases in the context helped you infer the meaning of these imaginary words? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61611916-38b4-4316-906e-1ea712b33bb2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "What you did in the above activity is referred to as **distributional hypothesis**. \n",
    "\n",
    "<blockquote> \n",
    "    <p>You shall know a word by the company it keeps.</p>\n",
    "    <footer>Firth, 1957</footer>        \n",
    "</blockquote>\n",
    "\n",
    "<blockquote> \n",
    "If A and B have almost identical environments we say that they are synonyms.\n",
    "<footer>Harris, 1954</footer>    \n",
    "</blockquote>    \n",
    "\n",
    "Example: \n",
    "\n",
    "- The plot twist was totally unexpected, making it a **flibbertigibbet** experience.\n",
    "- The plot twist was totally unexpected, making it a **delightful** experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400ac33f-68ae-4711-a70b-d80b86a83a4e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Word representations: intro \n",
    "- A standard way to represent meanings of words is by placing them into a vector space.\n",
    "- Distances between words in the vector space indicate relationships between them.\n",
    "   \n",
    "<!-- <img src=\"img/t-SNE_word_embeddings.png\" width=\"600\" height=\"600\"> -->\n",
    "\n",
    "![](../img/t-SNE_word_embeddings.png)\n",
    " \n",
    "(Attribution: [Jurafsky and Martin 3rd edition](https://web.stanford.edu/~jurafsky/slp3/))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456bcd27-c672-496f-be88-ba9bc93c29b7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Word meaning \n",
    "\n",
    "- A favourite topic of philosophers for centuries. \n",
    "- An example from legal domain: [Are hockey gloves \"gloves, mittens, mitts\" or \"articles of plastics\"?](https://www.scc-csc.ca/case-dossier/info/sum-som-eng.aspx?cas=36258)\n",
    "\n",
    "<blockquote>\n",
    "Canada (A.G.) v. Igloo Vikski Inc. was a tariff code case that made its way to the SCC (Supreme Court of Canada). The case disputed the definition of hockey gloves as either \"gloves, mittens, or mitts\" or as \"other articles of plastic.\"\n",
    "</blockquote>\n",
    "\n",
    "![](../img/hockey_gloves_case.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6ce503-71f0-4e7f-895a-1191936b87e9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Word meaning: ML and Natural Language Processing (NLP) view\n",
    "- Modeling word meaning that allows us to \n",
    "    - draw useful inferences to solve meaning-related problems \n",
    "    - find relationship between words, e.g., which words are similar, which ones have positive or negative connotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a350af69-aa6c-445f-9662-d7911a791694",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Word similarity   \n",
    "\n",
    "- Suppose you are carrying out sentiment analysis. \n",
    "- Consider the sentences below. \n",
    "\n",
    "> S1: This movie offers a **flibbertigibbet** blend of humour and action, a must-watch.\n",
    "\n",
    "> S2: This movie offers a **delightful** blend of humour and action, a must-watch.\n",
    "\n",
    "- Here we would like to capture similarity between **flibbertigibbet** and **delightful** in reference to sentiment analysis task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d39e2e-293b-4092-9489-9089d127d96a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### How are word embeddings related to unsupervised learning? \n",
    "\n",
    "- They are closely related to  extracting meaningful representations from raw data. \n",
    "- The word2vec algorithm is an unsupervised (or semi-supervised) method; we do not need any labeled data but we use running text as supervision signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fba7b20-4f85-44c0-a2fa-1953f01617e7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Overview of dot product and cosine similarity \n",
    "\n",
    "- To create a vector space where similar words are close together, we need some metric to measure distances between representations.\n",
    "- We have used the Euclidean distance before for numeric features.\n",
    "- For sparse features, the most commonly used metrics are Dot product and Cosine distance.\n",
    "- Let's look at an example. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb2bef5-6c7e-4538-ab1c-831908be7b24",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Euclidean distance\n",
    "\n",
    "$$distance(vec1, vec2) = \\sqrt{\\sum_{i =1}^{n} (vec1_i - vec2_i)^2}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73adf761",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vec1 = np.array([2.0, 4.0, 3.0])\n",
    "vec2 = np.array([5.0, 1.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6055a4e7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Distance: 5.1962\n"
     ]
    }
   ],
   "source": [
    "# Euclidean Distance\n",
    "euclidean_distance = np.linalg.norm(vec1 - vec2)\n",
    "print(f\"Euclidean Distance: {euclidean_distance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43df31a9-ebb4-435f-96f0-13e9fbd38e27",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Dot product similarity \n",
    "$$similarity_{dot product}(vec1,vec2) = vec1 \\cdot vec2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df796c24",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot Product: 14.0000\n"
     ]
    }
   ],
   "source": [
    "# Dot Product\n",
    "dot_product = np.dot(vec1, vec2)\n",
    "print(f\"Dot Product: {dot_product:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e1562d-dbe4-4da6-b6d0-c6a8c0f6e29e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Cosine similarity: \n",
    "\n",
    "- Normalized version of dot product.\n",
    "\n",
    "$$similarity_{cosine}(vec1,vec2) = \\frac{vec1.vec2}{\\left\\lVert vec1\\right\\rVert_2 \\left\\lVert vec2\\right\\rVert_2}$$\n",
    "\n",
    "Where, \n",
    "- The L2 norm of $vec1$ is the magnitude of $vec1$\n",
    "  $$\\left \\lVert vec1 \\right \\rVert_2 = \\sqrt{\\sum_i vec1_i^2}$$\n",
    "- The L2 norm of $vec2$ is the magnitude of $vec2$\n",
    "  $$\\left\\lVert vec2\\right\\rVert_2 = \\sqrt{\\sum_i vec2_i^2}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61f767fc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.5098\n"
     ]
    }
   ],
   "source": [
    "# Cosine Similarity\n",
    "cosine_similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "print(f\"Cosine Similarity: {cosine_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb2e8fc-5c58-45ca-b528-9c2f625c7a96",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Discussion question \n",
    "\n",
    "Suppose you are recommending items based on similarity between items. Given a query vector \"Query\" in the picture below and the three item vectors, determine the ranking for the three similarity measures below: \n",
    "- Similarity based on Euclidean distance\n",
    "- Similarity based on dot product\n",
    "- Cosine similarity\n",
    "\n",
    "<img src=\"../img/distance-metrics.png\" alt=\"\" height=\"800\" width=\"500\"> \n",
    "\n",
    "<!-- ![](../img/distance-metrics.svg) -->\n",
    "\n",
    "- Adapted from [here](https://developers.google.com/machine-learning/recommendation/overview/candidate-generation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3995a20d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Word embeddings\n",
    "- Word embeddings are dense vector representations of words that capture semantic relationships by positioning similar words close to each other in vector space. \n",
    "- By converting words into continuous numerical vectors, word embeddings allow machine learning models to work with text data effectively. \n",
    "- Some commonly used algorithms to create embeddings are `Word2Vec` or `GloVe`. They learn rich and meaningful representations of words from large corpora. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57c48a1-3be5-42eb-af50-379a735cb9bb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Pre-trained embeddings\n",
    "\n",
    "Creating these representations on your own is resource intensive. So people typically use \"pretrained\" embeddings. \n",
    "A number of pre-trained word embeddings are available. The most popular ones are:  \n",
    "\n",
    "- [word2vec](https://code.google.com/archive/p/word2vec/)\n",
    "    * trained on several corpora using the word2vec algorithm \n",
    "- [wikipedia2vec](https://wikipedia2vec.github.io/wikipedia2vec/pretrained/)\n",
    "    * pretrained embeddings for 12 languages \n",
    "- [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "    * trained using [the GloVe algorithm](https://nlp.stanford.edu/pubs/glove.pdf) \n",
    "    * published by Stanford University \n",
    "- [fastText pre-trained embeddings for 294 languages](https://fasttext.cc/docs/en/pretrained-vectors.html) \n",
    "    * trained using [the fastText algorithm](http://aclweb.org/anthology/Q17-1010)\n",
    "    * published by Facebook    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1520fda6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "### How to use pretrained embeddings\n",
    "\n",
    "Let's try Google News pre-trained embeddings.  \n",
    "\n",
    "- You can download pre-trained embeddings from their original source. \n",
    "- `Gensim` provides an api to conveniently load them. You can install `gensim` as follows. \n",
    "\n",
    "    ```conda install -c conda-forge gensim```\n",
    "\n",
    "\n",
    "> See notes if you have a problem importing gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba8f9a0-f530-4311-8814-eaec661a3cfd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Below I am loading word vectors trained on Google News corpus using this api. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdf2e6e0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  3000000\n"
     ]
    }
   ],
   "source": [
    "# It'll take a while to run this when you try it out for the first time.\n",
    "import gensim.downloader as api\n",
    "\n",
    "google_news_vectors = api.load(\"word2vec-google-news-300\")\n",
    "print(\"Size of vocabulary: \", len(google_news_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a4da24-18e1-479a-97f7-5f489db66f34",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "- `google_news_vectors` above has 300 dimensional word vectors for 3,000,000 unique words/phrases from Google news. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71355e2-0179-4b89-84bd-fe01809720b0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## What can we do with these word vectors?\n",
    "\n",
    "- Let's examine word vector for the word UBC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bafcac7-4a8e-445f-b509-1e8bb858bf55",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.3828125 , -0.18066406,  0.10644531,  0.4296875 ,  0.21582031,\n",
       "       -0.10693359,  0.13476562, -0.08740234, -0.14648438, -0.09619141,\n",
       "        0.02807617,  0.01409912, -0.12890625, -0.21972656, -0.41210938,\n",
       "       -0.1875    , -0.11914062, -0.22851562,  0.19433594, -0.08642578],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_vectors[\"UBC\"][:20]  # Representation of the word UBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ceaa820-1afe-4d2f-93aa-4b7196a324df",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_vectors[\"UBC\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec9544f-7edc-45ae-ab05-c321737f98b2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "It's a short and a dense (we do not see any zeros) vector!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe55b79-456c-45e3-af1d-355530679df4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Finding similar words\n",
    "\n",
    "Given word $w$, search in the vector space for the word closest to $w$ as measured by cosine similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c544772-7ed9-4d48-93b2-779a8be5ffc9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('UVic', 0.788647472858429),\n",
       " ('SFU', 0.7588528394699097),\n",
       " ('Simon_Fraser', 0.7356574535369873),\n",
       " ('UFV', 0.6880435943603516),\n",
       " ('VIU', 0.6778583526611328),\n",
       " ('Kwantlen', 0.677142858505249),\n",
       " ('UBCO', 0.6734487414360046),\n",
       " ('UPEI', 0.6731126308441162),\n",
       " ('UBC_Okanagan', 0.6709135174751282),\n",
       " ('Lakehead_University', 0.6622507572174072)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_vectors.most_similar(\"UBC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa90322d-c10c-4475-9b6d-e75dd9fe79dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('info', 0.7363681793212891),\n",
       " ('infomation', 0.680029571056366),\n",
       " ('infor_mation', 0.673384964466095),\n",
       " ('informaiton', 0.6639009118080139),\n",
       " ('informa_tion', 0.660125732421875),\n",
       " ('informationon', 0.633933424949646),\n",
       " ('informationabout', 0.6320979595184326),\n",
       " ('Information', 0.6186580657958984),\n",
       " ('informaion', 0.6093292236328125),\n",
       " ('details', 0.6063088774681091)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_vectors.most_similar(\"information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e6083f-5633-447e-844a-26627b80de3a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Finding similarity scores between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf3ef727-0df3-425f-abd0-975aaca3af75",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27610132"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_vectors.similarity(\"Canada\", \"hockey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a46a862-840d-44b3-82d3-31a487c53bae",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001962787"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_vectors.similarity(\"Japan\", \"hockey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d0f6e57-fc5d-4b57-93e6-e6fa1b386a6a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between height and tall is 0.473\n",
      "The similarity between height and official is 0.002\n",
      "The similarity between pineapple and mango is 0.668\n",
      "The similarity between pineapple and juice is 0.418\n",
      "The similarity between sun and robot is 0.029\n",
      "The similarity between GPU and hummus is 0.094\n"
     ]
    }
   ],
   "source": [
    "word_pairs = [\n",
    "    (\"height\", \"tall\"),\n",
    "    (\"height\", \"official\"),\n",
    "    (\"pineapple\", \"mango\"),\n",
    "    (\"pineapple\", \"juice\"),\n",
    "    (\"sun\", \"robot\"),\n",
    "    (\"GPU\", \"hummus\"),\n",
    "]\n",
    "for pair in word_pairs:\n",
    "    print(\n",
    "        \"The similarity between %s and %s is %0.3f\"\n",
    "        % (pair[0], pair[1], google_news_vectors.similarity(pair[0], pair[1]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbf1fef-d3c2-4a9d-8e23-1d4f7a6fea9a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Success of word2vec\n",
    "\n",
    "- This analogy example often comes up when people talk about word2vec, which was used by the authors of this method. \n",
    "- **MAN : KING :: WOMAN : ?**\n",
    "    - What is the word that is similar to **WOMAN** in the same sense as **KING** is similar to **MAN**?\n",
    "- Perform a simple algebraic operations with the vector representation of words.\n",
    "    $\\vec{X} = \\vec{\\text{KING}} − \\vec{\\text{MAN}} + \\vec{\\text{WOMAN}}$\n",
    "- Search in the vector space for the word closest to $\\vec{X}$ measured by cosine distance.\n",
    "\n",
    "<img src=\"../img/word_analogies1.png\" width=\"400\" height=\"400\">\n",
    "    \n",
    "(Credit: Mikolov et al. 2013)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d3fcd2a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analogy(word1, word2, word3, model=google_news_vectors):\n",
    "    \"\"\"\n",
    "    Returns analogy word using the given model.\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    word1 : (str)\n",
    "        word1 in the analogy relation\n",
    "    word2 : (str)\n",
    "        word2 in the analogy relation\n",
    "    word3 : (str)\n",
    "        word3 in the analogy relation\n",
    "    model :\n",
    "        word embedding model\n",
    "\n",
    "    Returns\n",
    "    ---------------\n",
    "        pd.dataframe\n",
    "    \"\"\"\n",
    "    print(\"%s : %s :: %s : ?\" % (word1, word2, word3))\n",
    "    sim_words = model.most_similar(positive=[word3, word2], negative=[word1])\n",
    "    return pd.DataFrame(sim_words, columns=[\"Analogy word\", \"Score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a0c1b28-8dce-455d-a23a-cdad66a6914c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : king :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>queen</td>\n",
       "      <td>0.711819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>monarch</td>\n",
       "      <td>0.618967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>princess</td>\n",
       "      <td>0.590243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>crown_prince</td>\n",
       "      <td>0.549946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>prince</td>\n",
       "      <td>0.537732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kings</td>\n",
       "      <td>0.523684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Queen_Consort</td>\n",
       "      <td>0.523595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>queens</td>\n",
       "      <td>0.518113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sultan</td>\n",
       "      <td>0.509859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>monarchy</td>\n",
       "      <td>0.508741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Analogy word     Score\n",
       "0          queen  0.711819\n",
       "1        monarch  0.618967\n",
       "2       princess  0.590243\n",
       "3   crown_prince  0.549946\n",
       "4         prince  0.537732\n",
       "5          kings  0.523684\n",
       "6  Queen_Consort  0.523595\n",
       "7         queens  0.518113\n",
       "8         sultan  0.509859\n",
       "9       monarchy  0.508741"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"man\", \"king\", \"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dde817d0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Italy : pizza :: Japan : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sushi</td>\n",
       "      <td>0.542754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ramen</td>\n",
       "      <td>0.541916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bento</td>\n",
       "      <td>0.539965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>teriyaki</td>\n",
       "      <td>0.502705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yakisoba</td>\n",
       "      <td>0.502351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>takoyaki</td>\n",
       "      <td>0.501550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>onigiri</td>\n",
       "      <td>0.501540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>burger</td>\n",
       "      <td>0.495270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ramen_noodle</td>\n",
       "      <td>0.493126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>noodle</td>\n",
       "      <td>0.490753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Analogy word     Score\n",
       "0         sushi  0.542754\n",
       "1         ramen  0.541916\n",
       "2         bento  0.539965\n",
       "3      teriyaki  0.502705\n",
       "4      yakisoba  0.502351\n",
       "5      takoyaki  0.501550\n",
       "6       onigiri  0.501540\n",
       "7        burger  0.495270\n",
       "8  ramen_noodle  0.493126\n",
       "9        noodle  0.490753"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"Italy\", \"pizza\", \"Japan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd181b49-7a76-4808-bf78-f424ec1822f3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "So you can imagine these models being useful in many meaning-related tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53618abd-baa6-4847-9556-8b49b65469db",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "![](../img/word2vec-country-capitals.png)\n",
    "\n",
    "(Credit: Mikolov et al. 2013)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8752b760-ff7c-45dc-b05d-6825d62489a6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Examples of semantic and syntactic relationships\n",
    "\n",
    "<img src=\"../img/word_analogies2.png\" width=\"800\" height=\"800\">\n",
    "\n",
    "(Credit: Mikolov 2013)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f885c1-e9e5-449b-8316-255c04717cbf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Implicit biases and stereotypes in word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1558651a-21f0-4fbe-ba94-e09f80d7618c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : computer_programmer :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>homemaker</td>\n",
       "      <td>0.562712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>housewife</td>\n",
       "      <td>0.510505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>graphic_designer</td>\n",
       "      <td>0.505180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>schoolteacher</td>\n",
       "      <td>0.497949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>businesswoman</td>\n",
       "      <td>0.493489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>paralegal</td>\n",
       "      <td>0.492551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>registered_nurse</td>\n",
       "      <td>0.490797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>saleswoman</td>\n",
       "      <td>0.488163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>electrical_engineer</td>\n",
       "      <td>0.479773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mechanical_engineer</td>\n",
       "      <td>0.475540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Analogy word     Score\n",
       "0            homemaker  0.562712\n",
       "1            housewife  0.510505\n",
       "2     graphic_designer  0.505180\n",
       "3        schoolteacher  0.497949\n",
       "4        businesswoman  0.493489\n",
       "5            paralegal  0.492551\n",
       "6     registered_nurse  0.490797\n",
       "7           saleswoman  0.488163\n",
       "8  electrical_engineer  0.479773\n",
       "9  mechanical_engineer  0.475540"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"man\", \"computer_programmer\", \"woman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb12104d-04fc-4ec7-8bb5-97f85bc3eeaa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### A caution about word embeddings\n",
    "\n",
    "- Embeddings reflect gender stereotypes present in broader society.\n",
    "- They may also amplify these stereotypes because of their widespread usage. \n",
    "- See the paper [Man is to Computer Programmer as Woman is to ...](http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf).\n",
    "\n",
    "**Most of the modern embeddings are de-biased for some obvious biases.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fc51f4-a562-4cd3-9c53-b3a8c3862a9d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Other popular methods to get embeddings\n",
    "\n",
    "**[fastText](https://fasttext.cc/)**\n",
    "\n",
    "- NLP library by Facebook research  \n",
    "\n",
    "**(Optional) [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)**     \n",
    "\n",
    "> See notes for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2fec1c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Word vectors with spaCy\n",
    "\n",
    "- spaCy also gives you access to word vectors with bigger models: `en_core_web_md` or `en_core_web_lr`\n",
    "- `spaCy`'s pre-trained embeddings are trained on [OntoNotes corpus](https://catalog.ldc.upenn.edu/LDC2013T19).\n",
    "- This corpus has a collection of different styles of texts such as telephone conversations, newswire, newsgroups, broadcast news, broadcast conversation, weblogs, religious texts. \n",
    "- Let's try it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfb70598",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.63358,  0.12266,  0.47232, -0.22974, -0.26307,  0.56499,\n",
       "       -0.72338,  0.16736,  0.4203 ,  0.93788], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc = nlp(\"pineapple\") # extract all interesting information about the document\n",
    "doc.vector[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4551824",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a327ee26",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Representing documents using word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5802b880",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "- Assuming that we have reasonable representations of words. \n",
    "- How do we represent meaning of paragraphs or documents?\n",
    "- Two simple approaches\n",
    "    - Averaging embeddings\n",
    "    - Concatenating embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f479656a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Averaging embeddings\n",
    "\n",
    "<blockquote>\n",
    "All empty promises\n",
    "</blockquote>\n",
    "    \n",
    "$(embedding(all) + embedding(empty) + embedding(promise))/3$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733e3037",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Average embeddings with \n",
    "\n",
    "We can get average embeddings for a sentence or a document in `spaCy` as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee03b46a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "Vector for: All empty promises\n",
      "[-0.6622033   0.06859333  0.18747167  0.04704666 -0.239323    0.043686\n",
      "  0.13470568 -0.00746667 -0.03796467  1.9193001 ]\n"
     ]
    }
   ],
   "source": [
    "s = \"All empty promises\"\n",
    "doc = nlp(s)\n",
    "avg_sent_emb = doc.vector\n",
    "print(avg_sent_emb.shape)\n",
    "print(\"Vector for: {}\\n{}\".format((s), (avg_sent_emb[0:10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a393fa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "Check out [Appendix_B](Appendix-B.ipynb) to see an example of using these embeddings for text classification. That said, compared to these, the sentence transformers provide better representations for sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a13741",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Break (5 min)\n",
    "\n",
    "![](../img/eva-coffee.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda14ef9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Topic modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a86d845",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "### Why topic modeling?\n",
    "\n",
    "#### Topic modeling introduction activity (~5 mins)\n",
    "\n",
    "- Consider the following documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a064418",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elegant fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fashion model at famous probabilistic topic mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fresh elegant fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>famous elegant fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>probabilistic conference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>creative probabilistic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>model diet apple kiwi nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>probabilistic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kiwi health nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fresh apple kiwi health diet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>health nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>fresh apple kiwi juice nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>probabilistic topic model conference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>probabilistic topi model</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "0                                famous fashion model\n",
       "1                               elegant fashion model\n",
       "2   fashion model at famous probabilistic topic mo...\n",
       "3                         fresh elegant fashion model\n",
       "4                       famous elegant fashion model \n",
       "5                            probabilistic conference\n",
       "6                        creative probabilistic model\n",
       "7                     model diet apple kiwi nutrition\n",
       "8                                 probabilistic model\n",
       "9                               kiwi health nutrition\n",
       "10                      fresh apple kiwi health diet \n",
       "11                                   health nutrition\n",
       "12                   fresh apple kiwi juice nutrition\n",
       "13               probabilistic topic model conference\n",
       "14                           probabilistic topi model"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_df = pd.read_csv(\"../data/toy_clustering.csv\")\n",
    "toy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17d6f5c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "1. Suppose you are asked to cluster these documents manually. How many clusters would you identify?\n",
    "2. What are the prominent words in each cluster? \n",
    "3. Are there documents which are a mixture of multiple clusters? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "839fee22-896e-43fe-b9dc-816a9a31cc6f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elegant fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fashion model at famous probabilistic topic mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fresh elegant fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>famous elegant fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>probabilistic conference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>creative probabilistic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>model diet apple kiwi nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>probabilistic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kiwi health nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fresh apple kiwi health diet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>health nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>fresh apple kiwi juice nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>probabilistic topic model conference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>probabilistic topi model</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "0                                famous fashion model\n",
       "1                               elegant fashion model\n",
       "2   fashion model at famous probabilistic topic mo...\n",
       "3                         fresh elegant fashion model\n",
       "4                       famous elegant fashion model \n",
       "5                            probabilistic conference\n",
       "6                        creative probabilistic model\n",
       "7                     model diet apple kiwi nutrition\n",
       "8                                 probabilistic model\n",
       "9                               kiwi health nutrition\n",
       "10                      fresh apple kiwi health diet \n",
       "11                                   health nutrition\n",
       "12                   fresh apple kiwi juice nutrition\n",
       "13               probabilistic topic model conference\n",
       "14                           probabilistic topi model"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcead49",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Topic modeling motivation\n",
    "\n",
    "- Humans are pretty good at reading and understanding a document and answering questions such as \n",
    "    - What is it about?  \n",
    "    - Which documents is it related to?     \n",
    "- What if you're given a large collection of documents on a variety of topics.      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9232b0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Example: A corpus of news articles \n",
    "\n",
    "![](../img/TM_NYT_articles.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/TM_NYT_articles.png\" height=\"2000\" width=\"2000\">  -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7691722",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Example: A corpus of food magazines \n",
    "\n",
    "![](../img/TM_food_magazines.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/TM_food_magazines.png\" height=\"2000\" width=\"2000\">  -->\n",
    "<!-- </center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a6c8ff",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### A corpus of scientific articles\n",
    "\n",
    "![](../img/TM_science_articles.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/TM_science_articles.png\" height=\"2000\" width=\"2000\">  -->\n",
    "<!-- </center> -->\n",
    "\n",
    "(Credit: [Dave Blei's presentation](http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dd6136",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "- It would take years to read all documents and organize and categorize them so that they are easy to search.\n",
    "- You need an automated way\n",
    "    - to get an idea of what's going on in the data or \n",
    "    - to pull documents related to a certain topic\n",
    "- **Topic modeling** gives you an ability to summarize the major themes in a large collection of documents (corpus). \n",
    "    - Example: The major themes in a collection of news articles could be \n",
    "        - **politics**\n",
    "        - **entertainment**\n",
    "        - **sports**\n",
    "        - **technology**\n",
    "        - ...    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9869ff",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### How do you do topic modeling? \n",
    "\n",
    "- A common tool to solve such problems is **unsupervised ML methods**.\n",
    "- Given the hyperparameter $K$, the goal of topic modeling is to describe a set of documents using $K$ \"topics\".\n",
    "- In unsupervised setting, the input of topic modeling is \n",
    "    - A large collection of documents\n",
    "    - A value for the hyperparameter $K$ (e.g., $K = 3$)\n",
    "- and the output is \n",
    "    1. Topic-words association \n",
    "        - For each topic, what words describe that topic? \n",
    "    2. Document-topics association\n",
    "        - For each document, what topics are expressed by the document? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b437faf0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Topic modeling: Example\n",
    "\n",
    "- Topic-words association \n",
    "    - For each topic, what words describe that topic?  \n",
    "    - A topic is a mixture of words. \n",
    "\n",
    "![](../img/topic_modeling_word_topics.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/topic_modeling_word_topics.png\" height=\"1000\" width=\"1000\">  -->\n",
    "<!-- </center> -->    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6e0004",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Topic modeling: Example\n",
    "\n",
    "- Document-topics association \n",
    "    - For each document, what topics are expressed by the document?\n",
    "    - A document is a mixture of topics. \n",
    "    \n",
    "![](../img/topic_modeling_doc_topics.png)\n",
    "\n",
    "<!-- <center> -->    \n",
    "<!-- <img src=\"img/topic_modeling_doc_topics.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center> -->    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daf9e61",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Topic modeling: Input and output\n",
    "\n",
    "- Input\n",
    "    - A large collection of documents\n",
    "    - A value for the hyperparameter $K$ (e.g., $K = 3$)\n",
    "- Output\n",
    "    - For each topic, what words describe that topic?  \n",
    "    - For each document, what topics are expressed by the document?\n",
    "\n",
    "![](../img/topic_modeling_output.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/topic_modeling_output.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center> -->    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477da699",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Topic modeling: Some applications\n",
    "\n",
    "- Topic modeling is a great EDA tool to get a sense of what's going on in a large corpus. \n",
    "- Some examples\n",
    "    - If you want to pull documents related to a particular lawsuit. \n",
    "    - You want to examine people's sentiment towards a particular candidate and/or political party and so you want to pull tweets or Facebook posts related to election.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68403d4a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Topic modeling examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095a4eda",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "### Topic modeling: Input \n",
    "\n",
    "![](../img/TM_science_articles.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/TM_science_articles.png\" height=\"2000\" width=\"2000\">  -->\n",
    "<!-- </center>     -->\n",
    "    \n",
    "Credit: [David Blei's presentation](http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f996d0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Topic modeling: output\n",
    "\n",
    "![](../img/TM_topics.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/TM_topics.png\" height=\"900\" width=\"900\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "\n",
    "(Credit: [David Blei's presentation](http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75de30fb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Topic modeling: output with interpretation\n",
    "- Assigning labels is a human thing. \n",
    "\n",
    "![](../img/TM_topics_with_labels.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/TM_topics_with_labels.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "(Credit: [David Blei's presentation](http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e108486",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### LDA topics in Yale Law Journal\n",
    "\n",
    "![](../img/TM_yale_law_journal.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/TM_yale_law_journal.png\" height=\"1500\" width=\"1500\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "(Credit: [David Blei's paper](http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61296bc2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### LDA topics in social media\n",
    "\n",
    "![](../img/TM_health_topics_social_media.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/TM_health_topics_social_media.png\" height=\"1300\" width=\"1300\">  -->\n",
    "<!-- </center> -->\n",
    "\n",
    "(Credit: [Health topics in social media](https://journals.plos.org/plosone/article/figure?id=10.1371/journal.pone.0103408.g002))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360ce690",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Topic modeling toy example\n",
    "\n",
    "In this lecture, I will demonstrate how to perform topic modeling using the **Latent Dirichlet Allocation** model implemented in `sklearn`. We won't delve into the inner workings of the model, as it falls outside the scope of this course. Instead, our objective is to understand how to apply it to your specific problems and comprehend the model's input and output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6750870-f2ad-4197-8dfd-e319130c0a7a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Let's work with a toy example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e1194b4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>fashion model pattern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>fashion model probabilistic topic model confer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>fresh fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>creative fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>fashion model probabilistic topic model confer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>probabilistic model pattern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>fashion model probabilistic topic model confer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>apple kiwi nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>kiwi health nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>fresh apple health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>creative health nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>hidden markov model probabilistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>apple kiwi nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>apple kiwi health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>apple kiwi nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>fresh kiwi health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>apple kiwi nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>apple kiwi nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>apple kiwi nutrition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_id                                               text\n",
       "0        1                               famous fashion model\n",
       "1        2                             fashion model pattern \n",
       "2        3  fashion model probabilistic topic model confer...\n",
       "3        4                               famous fashion model\n",
       "4        5                                fresh fashion model\n",
       "5        6                               famous fashion model\n",
       "6        7                               famous fashion model\n",
       "7        8                               famous fashion model\n",
       "8        9                               famous fashion model\n",
       "9       10                             creative fashion model\n",
       "10      11                               famous fashion model\n",
       "11      12                               famous fashion model\n",
       "12      13  fashion model probabilistic topic model confer...\n",
       "13      14                          probabilistic topic model\n",
       "14      15                        probabilistic model pattern\n",
       "15      16                          probabilistic topic model\n",
       "16      17                          probabilistic topic model\n",
       "17      18                          probabilistic topic model\n",
       "18      19                          probabilistic topic model\n",
       "19      20                          probabilistic topic model\n",
       "20      21                          probabilistic topic model\n",
       "21      22  fashion model probabilistic topic model confer...\n",
       "22      23                               apple kiwi nutrition\n",
       "23      24                              kiwi health nutrition\n",
       "24      25                                 fresh apple health\n",
       "25      26                          probabilistic topic model\n",
       "26      27                          creative health nutrition\n",
       "27      28                          probabilistic topic model\n",
       "28      29                          probabilistic topic model\n",
       "29      30                  hidden markov model probabilistic\n",
       "30      31                          probabilistic topic model\n",
       "31      32                          probabilistic topic model\n",
       "32      33                               apple kiwi nutrition\n",
       "33      34                                  apple kiwi health\n",
       "34      35                               apple kiwi nutrition\n",
       "35      36                                  fresh kiwi health\n",
       "36      37                               apple kiwi nutrition\n",
       "37      38                               apple kiwi nutrition\n",
       "38      39                               apple kiwi nutrition"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_df = pd.read_csv(\"../data/toy_lda_data.csv\")\n",
    "toy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee72569",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Let's carry out topic modeling using a popular model called **Latent Dirichlet Allocation (LDA)**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723872a8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "- Input to the LDA topic model is bag-of-words representation of text.\n",
    "- Let's create bag-of-words representation of \"text\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d48ea2d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<39x15 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 124 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(stop_words=\"english\")\n",
    "toy_X = vec.fit_transform(toy_df[\"text\"])\n",
    "toy_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c577718",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['apple', 'conference', 'creative', 'famous', 'fashion', 'fresh',\n",
       "       'health', 'hidden', 'kiwi', 'markov', 'model', 'nutrition',\n",
       "       'pattern', 'probabilistic', 'topic'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = vec.get_feature_names_out() # vocabulary\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1746c414",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "070df643",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_topics = 3 # number of topics\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics, learning_method=\"batch\", max_iter=10, random_state=0\n",
    ")\n",
    "lda.fit(toy_X) \n",
    "document_topics = lda.transform(toy_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726c5aa5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "- Once we have a fitted model we can get the word-topic association and document-topic association  \n",
    "- Word-topic association\n",
    "    - `lda.components_` gives us the weights associated with each word for each topic. In other words, it tells us which word is important for which topic. \n",
    "- Document-topic association    \n",
    "    - Calling transform on the data gives us document-topic association. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a46d985a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.33380754,  3.31038074,  0.33476534,  0.33397112,  0.36695134,\n",
       "         0.33439238,  0.33381373,  0.35771821,  0.33380649,  0.35771821,\n",
       "        17.78521263,  0.33380761,  0.3573886 , 17.31634363, 15.32791718],\n",
       "       [ 8.33224516,  0.33400489,  2.2173627 ,  0.33411086,  0.33732465,\n",
       "         3.28753559,  5.33223002,  0.33435326,  9.33224759,  0.33435326,\n",
       "         0.33797555,  8.3322447 ,  0.33462759,  0.33440682,  0.33425967],\n",
       "       [ 0.3339473 ,  0.35561437,  0.44787197,  8.33191802, 14.29572402,\n",
       "         0.37807203,  0.33395626,  1.30792853,  0.33394593,  1.30792853,\n",
       "        13.87681182,  0.33394769,  2.30798381,  0.34924955,  0.33782315]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23eea8d8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lda.components_.shape: (3, 15)\n"
     ]
    }
   ],
   "source": [
    "print(\"lda.components_.shape: {}\".format(lda.components_.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3685f159",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_lda_w_vectors(W, component_labels, feature_names, width=800, height=600): \n",
    "    \n",
    "    fig = pp.imshow(\n",
    "        W,\n",
    "    )\n",
    "    #     y=component_labels,\n",
    "    #     x=feature_names,\n",
    "    #     color_continuous_scale=\"viridis\",\n",
    "    # )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Features\",\n",
    "        yaxis_title=\"Topics\",\n",
    "        xaxis = {'side': 'top',  'tickangle':300}, \n",
    "    )\n",
    "    fig.update_layout(\n",
    "        autosize=False,\n",
    "        width=width,\n",
    "        height=height,\n",
    "    )    \n",
    "\n",
    "    # return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c93c518f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(50.581597222222214, 0.5, 'Topic')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_df = pd.DataFrame(\n",
    "    lda.components_, \n",
    "    index=[\"topic_{}\".format(i) for i in range(3)], \n",
    "    columns=vocab\n",
    ")\n",
    "ax = sb.heatmap(plot_df, cbar=False)\n",
    "ax.set_xlabel(\"Feature\")\n",
    "ax.set_ylabel(\"Topic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f2f945",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Let's look at the words with highest weights for each topic more systematically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71a9648b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8,  0, 11,  6,  3,  5,  2, 12,  7,  9,  4,  1, 14, 13, 10],\n",
       "       [ 1,  3, 14,  7,  9, 13, 12,  4, 10,  2,  5,  6, 11,  0,  8],\n",
       "       [ 8,  0, 11,  6, 14, 13,  1,  5,  2,  9,  7, 12,  3, 10,  4]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(lda.components_, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e56a4543",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "feature_names = np.array(vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1608bbf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       \n",
      "--------      --------      --------      \n",
      "model         kiwi          fashion       \n",
      "probabilistic apple         model         \n",
      "topic         nutrition     famous        \n",
      "conference    health        pattern       \n",
      "fashion       fresh         hidden        \n",
      "markov        creative      markov        \n",
      "hidden        model         creative      \n",
      "pattern       fashion       fresh         \n",
      "creative      pattern       conference    \n",
      "fresh         probabilistic probabilistic \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mglearn\n",
    "\n",
    "mglearn.tools.print_topics(\n",
    "    topics=range(3),\n",
    "    feature_names=feature_names,\n",
    "    sorting=sorting,\n",
    "    topics_per_chunk=5,\n",
    "    n_words=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b598986",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "- Here is how we can interpret the topics\n",
    "    - Topic 0 $\\rightarrow$ ML modeling \n",
    "    - Topic 1 $\\rightarrow$ fruit and nutrition\n",
    "    - Topic 2 $\\rightarrow$ fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29ff62c6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'famous fashion model'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_df.iloc[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39d9130c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08791477, 0.08338644, 0.82869879])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_topics[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b49795",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "This document is made up of \n",
    "- ~83% topic 2\n",
    "- ~9% topic 0 \n",
    "- ~8% topic 1.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982cdcce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Topic modeling pipeline \n",
    "\n",
    "- Above we worked with toy data. In the real world, we usually need to preprocess the data before passing it to LDA. \n",
    "- Here are typical steps if you want to carry out topic modeling on real-world data. \n",
    "    - Preprocess your corpus. \n",
    "    - Train LDA.\n",
    "    - Interpret your topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e358441",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e891945",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wiki query</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>Artificial intelligence (AI) refers to the cap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unsupervised learning</td>\n",
       "      <td>In machine learning, supervised learning (SL) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Supreme Court of Canada</td>\n",
       "      <td>The Supreme Court of Canada (SCC; French: Cour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Peace, Order, and Good Government</td>\n",
       "      <td>In many Commonwealth jurisdictions, the phrase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Canadian constitutional law</td>\n",
       "      <td>Canadian constitutional law (French: droit con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ice hockey</td>\n",
       "      <td>Ice hockey (or simply hockey in North America)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          wiki query  \\\n",
       "0            Artificial Intelligence   \n",
       "1              unsupervised learning   \n",
       "2            Supreme Court of Canada   \n",
       "3  Peace, Order, and Good Government   \n",
       "4        Canadian constitutional law   \n",
       "5                         ice hockey   \n",
       "\n",
       "                                                text  \n",
       "0  Artificial intelligence (AI) refers to the cap...  \n",
       "1  In machine learning, supervised learning (SL) ...  \n",
       "2  The Supreme Court of Canada (SCC; French: Cour...  \n",
       "3  In many Commonwealth jurisdictions, the phrase...  \n",
       "4  Canadian constitutional law (French: droit con...  \n",
       "5  Ice hockey (or simply hockey in North America)...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wikipedia\n",
    "\n",
    "queries = [\n",
    "    \"Artificial Intelligence\",\n",
    "    \"unsupervised learning\",\n",
    "    \"Supreme Court of Canada\",\n",
    "    \"Peace, Order, and Good Government\",\n",
    "    \"Canadian constitutional law\",\n",
    "    \"ice hockey\",\n",
    "]\n",
    "wiki_dict = {\"wiki query\": [], \"text\": []}\n",
    "for i in range(len(queries)):\n",
    "    wiki_dict[\"text\"].append(wikipedia.page(queries[i]).content)\n",
    "    wiki_dict[\"wiki query\"].append(queries[i])\n",
    "\n",
    "wiki_df = pd.DataFrame(wiki_dict)\n",
    "wiki_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec790ae",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Preprocessing the corpus \n",
    "\n",
    "- **Preprocessing is crucial!**\n",
    "- Tokenization, converting text to lower case\n",
    "- Removing punctuation and stopwords\n",
    "- Discarding words with length < threshold or word frequency < threshold        \n",
    "- Possibly lemmatization: Consider the lemmas instead of inflected forms. \n",
    "- Depending upon your application, restrict to specific part of speech;\n",
    "    * For example, only consider nouns, verbs, and adjectives\n",
    "    \n",
    "We'll use [`spaCy`](https://spacy.io/) for preprocessing. Check out available token attributes [here](https://spacy.io/usage/rule-based-matching#adding-patterns-attributes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a43f9dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\", disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "272368dd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "    doc,\n",
    "    min_token_len=2,\n",
    "    irrelevant_pos=[\"ADV\", \"PRON\", \"CCONJ\", \"PUNCT\", \"PART\", \"DET\", \"ADP\", \"SPACE\"],\n",
    "):\n",
    "    \"\"\"\n",
    "    Given text, min_token_len, and irrelevant_pos carry out preprocessing of the text\n",
    "    and return a preprocessed string.\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    doc : (spaCy doc object)\n",
    "        the spacy doc object of the text\n",
    "    min_token_len : (int)\n",
    "        min_token_length required\n",
    "    irrelevant_pos : (list)\n",
    "        a list of irrelevant pos tags\n",
    "\n",
    "    Returns\n",
    "    -------------\n",
    "    (str) the preprocessed text\n",
    "    \"\"\"\n",
    "\n",
    "    clean_text = []\n",
    "\n",
    "    for token in doc:\n",
    "        if (\n",
    "            token.is_stop == False  # Check if it's not a stopword\n",
    "            and len(token) > min_token_len  # Check if the word meets minimum threshold\n",
    "            and token.pos_ not in irrelevant_pos\n",
    "        ):  # Check if the POS is in the acceptable POS tags\n",
    "            lemma = token.lemma_  # Take the lemma of the word\n",
    "            clean_text.append(lemma.lower())\n",
    "    return \" \".join(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "464785a5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wiki_df[\"text_pp\"] = [preprocess(text) for text in nlp.pipe(wiki_df[\"text\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4250717d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wiki query</th>\n",
       "      <th>text</th>\n",
       "      <th>text_pp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>Artificial intelligence (AI) refers to the cap...</td>\n",
       "      <td>artificial intelligence refer capability compu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unsupervised learning</td>\n",
       "      <td>In machine learning, supervised learning (SL) ...</td>\n",
       "      <td>machine learning supervised learning paradigm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Supreme Court of Canada</td>\n",
       "      <td>The Supreme Court of Canada (SCC; French: Cour...</td>\n",
       "      <td>supreme court canada scc french cour suprême c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Peace, Order, and Good Government</td>\n",
       "      <td>In many Commonwealth jurisdictions, the phrase...</td>\n",
       "      <td>commonwealth jurisdiction phrase peace order g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Canadian constitutional law</td>\n",
       "      <td>Canadian constitutional law (French: droit con...</td>\n",
       "      <td>canadian constitutional law french droit const...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ice hockey</td>\n",
       "      <td>Ice hockey (or simply hockey in North America)...</td>\n",
       "      <td>ice hockey hockey north america team sport pla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          wiki query  \\\n",
       "0            Artificial Intelligence   \n",
       "1              unsupervised learning   \n",
       "2            Supreme Court of Canada   \n",
       "3  Peace, Order, and Good Government   \n",
       "4        Canadian constitutional law   \n",
       "5                         ice hockey   \n",
       "\n",
       "                                                text  \\\n",
       "0  Artificial intelligence (AI) refers to the cap...   \n",
       "1  In machine learning, supervised learning (SL) ...   \n",
       "2  The Supreme Court of Canada (SCC; French: Cour...   \n",
       "3  In many Commonwealth jurisdictions, the phrase...   \n",
       "4  Canadian constitutional law (French: droit con...   \n",
       "5  Ice hockey (or simply hockey in North America)...   \n",
       "\n",
       "                                             text_pp  \n",
       "0  artificial intelligence refer capability compu...  \n",
       "1  machine learning supervised learning paradigm ...  \n",
       "2  supreme court canada scc french cour suprême c...  \n",
       "3  commonwealth jurisdiction phrase peace order g...  \n",
       "4  canadian constitutional law french droit const...  \n",
       "5  ice hockey hockey north america team sport pla...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa143f60",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Topic modeling with `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "12050384",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(stop_words='english')\n",
    "X = vec.fit_transform(wiki_df[\"text_pp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2dad9951",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_topics = 3\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics, learning_method=\"batch\", max_iter=10, random_state=0\n",
    ")\n",
    "document_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a662c94c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lda.components_.shape: (3, 4024)\n"
     ]
    }
   ],
   "source": [
    "print(\"lda.components_.shape: {}\".format(lda.components_.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf251037",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       \n",
      "--------      --------      --------      \n",
      "1953          hockey        court         \n",
      "intention     player        power         \n",
      "mobility      ice           law           \n",
      "danger        team          learning      \n",
      "complement    league        intelligence  \n",
      "retain        play          algorithm     \n",
      "elect         puck          machine       \n",
      "discretion    game          government    \n",
      "attack        penalty       problem       \n",
      "organize      nhl           datum         \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "feature_names = np.array(vec.get_feature_names_out())\n",
    "mglearn.tools.print_topics(\n",
    "    topics=range(3),\n",
    "    feature_names=feature_names,\n",
    "    sorting=sorting,\n",
    "    topics_per_chunk=5,\n",
    "    n_words=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ff4816",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Check out some recent topic modeling tools\n",
    "- [Topic2Vec](https://top2vec.readthedocs.io/en/stable/Top2Vec.html)\n",
    "- [BERTopic](https://maartengr.github.io/BERTopic/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f70bfb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Topic modelline outside NLP\n",
    "\n",
    "- Topic modelling ideas and tools can be adapted outside NLP\n",
    "- Mutation signatures in cancer are one popular use [COSMIC](https://cancer.sanger.ac.uk/signatures/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76184200",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Basic text preprocessing [[video](https://youtu.be/7W5Q8gzNPBc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafeaf6a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "### Introduction \n",
    "- Why do we need preprocessing?\n",
    "    - Text data is unstructured and messy. \n",
    "    - We need to \"normalize\" it before we do anything interesting with it. \n",
    "- Example:     \n",
    "    - **Lemma**: Same stem, same part-of-speech, roughly the same meaning\n",
    "        - Vancouver's &rarr; Vancouver\n",
    "        - computers &rarr; computer \n",
    "        - rising &rarr; rise, rose, rises    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3ada02",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Tokenization\n",
    "\n",
    "- Sentence segmentation\n",
    "    - Split text into sentences\n",
    "- Word tokenization \n",
    "    - Split sentences into words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0707a841",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Tokenization: sentence segmentation\n",
    "\n",
    "<blockquote>\n",
    "MDS is a Master's program at UBC in British Columbia. MDS teaching team is truly multicultural!! Dr. George did his Ph.D. in Scotland. Dr. Timbers, Dr. Ostblom, Dr. Rodríguez-Arelis, and Dr. Kolhatkar did theirs in Canada. Dr. Gelbart did his PhD in the U.S.\n",
    "</blockquote>\n",
    "\n",
    "- How many sentences are there in this text? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fe237aca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UBC is one of the well known universities in British Columbia', ' UBC CS teaching team is truly multicultural!! Dr', ' Toti completed her Ph', 'D', ' in Italy', 'Dr', ' Moosvi, Dr', ' Kolhatkar, Dr', ' Ola and Dr', ' Roth completed theirs in Canada', 'Dr', ' Heeren and Dr', ' Lécuyer completed theirs in the U', 'S', '']\n"
     ]
    }
   ],
   "source": [
    "### Let's do sentence segmentation on \".\"\n",
    "text = (\n",
    "    \"UBC is one of the well known universities in British Columbia. \"\n",
    "    \"UBC CS teaching team is truly multicultural!! \"\n",
    "    \"Dr. Toti completed her Ph.D. in Italy.\"\n",
    "    \"Dr. Moosvi, Dr. Kolhatkar, Dr. Ola and Dr. Roth completed theirs in Canada.\"\n",
    "    \"Dr. Heeren and Dr. Lécuyer completed theirs in the U.S.\"\n",
    ")\n",
    "print(text.split(\".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087261f4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Sentence segmentation\n",
    "\n",
    "- In English, period (.) is quite ambiguous. (In Chinese, it is unambiguous.)\n",
    "    - Abbreviations like Dr., U.S., Inc.  \n",
    "    - Numbers like 60.44%, 0.98\n",
    "- ! and ? are relatively ambiguous.\n",
    "- How about writing regular expressions? \n",
    "- A common way is using off-the-shelf models for sentence segmentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8c7ecbce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UBC is one of the well known universities in British Columbia.', 'UBC CS teaching team is truly multicultural!!', 'Dr. Toti completed her Ph.D. in Italy.Dr.', 'Moosvi, Dr. Kolhatkar, Dr. Ola and Dr. Roth completed theirs in Canada.Dr.', 'Heeren and Dr. Lécuyer completed theirs in the U.S.']\n"
     ]
    }
   ],
   "source": [
    "### Let's try to do sentence segmentation using nltk\n",
    "#nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokenized = sent_tokenize(text)\n",
    "print(sent_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae94967",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Word tokenization\n",
    "\n",
    "<blockquote>\n",
    "MDS is a Master's program at UBC in British Columbia. \n",
    "</blockquote>\n",
    "\n",
    "- How many words are there in this sentence?  \n",
    "- Is whitespace a sufficient condition for a word boundary?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307557c9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Word tokenization \n",
    "\n",
    "<blockquote>\n",
    "MDS is a Master's program at UBC in British Columbia. \n",
    "</blockquote>\n",
    "\n",
    "- What's our definition of a word?\n",
    "    - Should British Columbia be one word or two words? \n",
    "    - Should punctuation be considered a separate word?\n",
    "    - What about the punctuations in `U.S.`?\n",
    "    - What do we do with words like `Master's`?\n",
    "- This process of identifying word boundaries is referred to as **tokenization**.\n",
    "- You can use regex but better to do it with off-the-shelf ML models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c445a070",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting on whitespace:  [['UBC', 'is', 'one', 'of', 'the', 'well', 'known', 'universities', 'in', 'British', 'Columbia.'], ['UBC', 'CS', 'teaching', 'team', 'is', 'truly', 'multicultural!!'], ['Dr.', 'Toti', 'completed', 'her', 'Ph.D.', 'in', 'Italy.Dr.'], ['Moosvi,', 'Dr.', 'Kolhatkar,', 'Dr.', 'Ola', 'and', 'Dr.', 'Roth', 'completed', 'theirs', 'in', 'Canada.Dr.'], ['Heeren', 'and', 'Dr.', 'Lécuyer', 'completed', 'theirs', 'in', 'the', 'U.S.']]\n",
      "\n",
      "\n",
      "\n",
      "Tokenized:  [['UBC', 'is', 'one', 'of', 'the', 'well', 'known', 'universities', 'in', 'British', 'Columbia', '.'], ['UBC', 'CS', 'teaching', 'team', 'is', 'truly', 'multicultural', '!', '!'], ['Dr.', 'Toti', 'completed', 'her', 'Ph.D.', 'in', 'Italy.Dr', '.'], ['Moosvi', ',', 'Dr.', 'Kolhatkar', ',', 'Dr.', 'Ola', 'and', 'Dr.', 'Roth', 'completed', 'theirs', 'in', 'Canada.Dr', '.'], ['Heeren', 'and', 'Dr.', 'Lécuyer', 'completed', 'theirs', 'in', 'the', 'U.S', '.']]\n"
     ]
    }
   ],
   "source": [
    "### Let's do word segmentation on white spaces\n",
    "print(\"Splitting on whitespace: \", [sent.split() for sent in sent_tokenized])\n",
    "\n",
    "### Let's try to do word segmentation using nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenized = [word_tokenize(sent) for sent in sent_tokenized]\n",
    "# This is similar to the input format of word2vec algorithm\n",
    "print(\"\\n\\n\\nTokenized: \", word_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c00f06",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Word segmentation \n",
    "\n",
    "For some languages you need much more sophisticated tokenizers. \n",
    "- For languages such as Chinese, there are no spaces between words.\n",
    "    - [jieba](https://github.com/fxsjy/jieba) is a popular tokenizer for Chinese. \n",
    "- German doesn't separate compound words.\n",
    "    * Example: _rindfleischetikettierungsüberwachungsaufgabenübertragungsgesetz_\n",
    "    * (the law for the delegation of monitoring beef labeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3ba41a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Types and tokens\n",
    "- Usually in NLP, we talk about \n",
    "    - **Type** an element in the vocabulary\n",
    "    - **Token** an instance of that type in running text \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ef2e4d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Exercise for you \n",
    "\n",
    "<blockquote>    \n",
    "UBC is located in the beautiful province of British Columbia. It's very close \n",
    "to the U.S. border. You'll get to the USA border in about 45 mins by car.     \n",
    "</blockquote>  \n",
    "\n",
    "- Consider the example above. \n",
    "    - How many types? (task dependent)\n",
    "    - How many tokens? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06046f36",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Other commonly used preprocessing steps\n",
    "\n",
    "- Punctuation and stopword removal\n",
    "- Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd23ff3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Punctuation and stopword removal\n",
    "\n",
    "- The most frequently occurring words in English are not very useful in many NLP tasks.\n",
    "    - Example: _the_ , _is_ , _a_ , and punctuation\n",
    "- Probably not very informative in many tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8389df9b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['be', 'couldn', 'our', 'or', \"needn't\", 'to', 'own', 'didn', 'hasn', 'the', 'how', 'for', 'ain', 'he', 'had', 'themselves', \"didn't\", 'under', 'nor', 'yourselves', 'isn', 'shan', 'why', 'few', 'needn', \"i'd\", 'won', 'been', \"i'm\", 'against', \"hadn't\", 'off', 'at', 'then', \"haven't\", 'during', 'from', 'her', \"it's\", 'both', 'very', 'on', 'this', \"wouldn't\", 'below', 'itself', 'no', 'other', \"don't\", 'my', 're', 'but', 'because', \"we'd\", 'mustn', 'ourselves', 'has', 'd', 'wasn', 'not', \"i've\", 'between', 'did', 'herself', 'hers', 'in', 'same', 'an', 'only', 'does', \"couldn't\", 'yourself', 'are', 'having', \"she'd\", 'were', 'now', 'such', 'do', 'just', 'once', \"they're\", 'into', 'they', 'their', \"i'll\", \"she's\", 'am', \"should've\", 'until', \"they'll\", 'will', 'doesn', 'here', \"you'll\", 'again', 'ours', 't', 'through', 'ma', 'by', 'y', 'its', 'myself', 'don', \"isn't\", 've', 'himself', 'all', \"it'll\", 's', 'wouldn', \"he'd\", 'after', 'before', \"wasn't\", 'can', 'his', \"they'd\", 'a', 'whom', \"won't\", 'your', 'if', \"we've\", 'hadn', 'you', 'we', \"we're\", 'him', 'me', \"they've\", 'too', 'any', 'down', 'is', \"shouldn't\", 'shouldn', 'mightn', \"that'll\", 'and', 'with', \"you'd\", 'about', 'aren', 'further', 'was', 'll', 'over', \"we'll\", 'it', \"mustn't\", \"aren't\", \"mightn't\", 'of', 'who', 'out', 'when', 'm', 'more', 'them', 'while', \"hasn't\", \"he'll\", 'most', 'some', 'above', 'as', 'each', 'doing', 'haven', 'should', 'than', \"doesn't\", 'up', \"weren't\", 'weren', \"shan't\", 'have', 'she', \"she'll\", 'i', 'there', 'where', 'being', \"it'd\", \"you've\", 'these', 'which', 'those', 'what', 'that', 'so', 'yours', \"he's\", \"you're\", 'o', 'theirs', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "# Let's use `nltk.stopwords`.\n",
    "# Add punctuations to the list.\n",
    "stop_words = list(set(stopwords.words(\"english\")))\n",
    "import string\n",
    "\n",
    "punctuation = string.punctuation\n",
    "stop_words += list(punctuation)\n",
    "# stop_words.extend(['``','`','br','\"',\"”\", \"''\", \"'s\"])\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7de45d99",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ubc', 'one', 'well', 'known', 'universities', 'british', 'columbia', 'ubc', 'cs', 'teaching', 'team', 'truly', 'multicultural', 'dr.', 'toti', 'completed', 'ph.d.', 'italy.dr', 'moosvi', 'dr.', 'kolhatkar', 'dr.', 'ola', 'dr.', 'roth', 'completed', 'canada.dr', 'heeren', 'dr.', 'lécuyer', 'completed', 'u.s']\n"
     ]
    }
   ],
   "source": [
    "### Get rid of stop words\n",
    "preprocessed = []\n",
    "for sent in word_tokenized:\n",
    "    for token in sent:\n",
    "        token = token.lower()\n",
    "        if token not in stop_words:\n",
    "            preprocessed.append(token)\n",
    "print(preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580ad39b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Lemmatization \n",
    "\n",
    "- For many NLP tasks (e.g., web search) we want to ignore morphological differences between words\n",
    "    - Example: If your search term is \"studying for ML quiz\" you might want to include pages containing \"tips to study for an ML quiz\" or \"here is how I studied for my ML quiz\"\n",
    "- Lemmatization converts inflected forms into the base form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2291c673",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/andrew/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ff0d4479",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/andrew/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "002d81bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma of studying:  study\n",
      "Lemma of studied:  study\n"
     ]
    }
   ],
   "source": [
    "# nltk has a lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(\"Lemma of studying: \", lemmatizer.lemmatize(\"studying\", \"v\"))\n",
    "print(\"Lemma of studied: \", lemmatizer.lemmatize(\"studied\", \"v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b50a70",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Stemming\n",
    "\n",
    "- Has a similar purpose but it is a crude chopping of affixes \n",
    "    * _automates, automatic, automation_ all reduced to _automat_.\n",
    "- Usually these reduced forms (stems) are not actual words themselves.  \n",
    "- A popular stemming algorithm for English is PorterStemmer. \n",
    "- Beware that it can be aggressive sometimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6aa0c3de",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before stemming:  UBC is located in the beautiful province of British Columbia... It's very close to the U.S. border.\n",
      "\n",
      "\n",
      "After stemming:  ubc is locat in the beauti provinc of british columbia ... it 's veri close to the u.s. border .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "text = (\n",
    "    \"UBC is located in the beautiful province of British Columbia... \"\n",
    "    \"It's very close to the U.S. border.\"\n",
    ")\n",
    "ps = PorterStemmer()\n",
    "tokenized = word_tokenize(text)\n",
    "stemmed = [ps.stem(token) for token in tokenized]\n",
    "print(\"Before stemming: \", text)\n",
    "print(\"\\n\\nAfter stemming: \", \" \".join(stemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec5d7ef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Other tools for preprocessing \n",
    "\n",
    "- We used [Natural Language Processing Toolkit (nltk)](https://www.nltk.org/) above\n",
    "- Many available tools    \n",
    "- [spaCy](https://spacy.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c8736d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### [spaCy](https://spacy.io/)\n",
    "\n",
    "- Industrial strength NLP library. \n",
    "- Lightweight, fast, and convenient to use. \n",
    "- spaCy does many things that we did above in one line of code! \n",
    "- Also has [multi-lingual](https://spacy.io/models/xx) support. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "065c8b23",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "text = (\n",
    "    \"MDS is a Master's program at UBC in British Columbia. \"\n",
    "    \"MDS teaching team is truly multicultural!! \"\n",
    "    \"Dr. George did his Ph.D. in Scotland. \"\n",
    "    \"Dr. Timbers, Dr. Ostblom, Dr. Rodríguez-Arelis, and Dr. Kolhatkar did theirs in Canada. \"\n",
    "    \"Dr. Gelbart did his PhD in the U.S.\"\n",
    ")\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3cd2da4b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens:  [MDS, is, a, Master, 's, program, at, UBC, in, British, Columbia, ., MDS, teaching, team, is, truly, multicultural, !, !, Dr., George, did, his, Ph.D., in, Scotland, ., Dr., Timbers, ,, Dr., Ostblom, ,, Dr., Rodríguez, -, Arelis, ,, and, Dr., Kolhatkar, did, theirs, in, Canada, ., Dr., Gelbart, did, his, PhD, in, the, U.S.]\n",
      "\n",
      "Lemmas:  ['MDS', 'be', 'a', 'Master', \"'s\", 'program', 'at', 'UBC', 'in', 'British', 'Columbia', '.', 'MDS', 'teaching', 'team', 'be', 'truly', 'multicultural', '!', '!', 'Dr.', 'George', 'do', 'his', 'ph.d.', 'in', 'Scotland', '.', 'Dr.', 'Timbers', ',', 'Dr.', 'Ostblom', ',', 'Dr.', 'Rodríguez', '-', 'Arelis', ',', 'and', 'Dr.', 'Kolhatkar', 'do', 'theirs', 'in', 'Canada', '.', 'Dr.', 'Gelbart', 'do', 'his', 'phd', 'in', 'the', 'U.S.']\n",
      "\n",
      "POS:  ['PROPN', 'AUX', 'DET', 'PROPN', 'PART', 'NOUN', 'ADP', 'PROPN', 'ADP', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'NOUN', 'NOUN', 'AUX', 'ADV', 'ADJ', 'PUNCT', 'PUNCT', 'PROPN', 'PROPN', 'VERB', 'PRON', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'CCONJ', 'PROPN', 'PROPN', 'VERB', 'PRON', 'ADP', 'PROPN', 'PUNCT', 'PROPN', 'PROPN', 'VERB', 'PRON', 'NOUN', 'ADP', 'DET', 'PROPN']\n"
     ]
    }
   ],
   "source": [
    "# Accessing tokens\n",
    "tokens = [token for token in doc]\n",
    "print(\"\\nTokens: \", tokens)\n",
    "\n",
    "# Accessing lemma\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(\"\\nLemmas: \", lemmas)\n",
    "\n",
    "# Accessing pos\n",
    "pos = [token.pos_ for token in doc]\n",
    "print(\"\\nPOS: \", pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63793860",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Other typical NLP tasks \n",
    "In order to understand text, we usually are interested in extracting information from text. Some common tasks in NLP pipeline are: \n",
    "- Part of speech tagging\n",
    "    - Assigning part-of-speech tags to all words in a sentence.\n",
    "- Named entity recognition\n",
    "    - Labelling named “real-world” objects, like persons, companies or locations.    \n",
    "- Coreference resolution\n",
    "    - Deciding whether two strings (e.g., UBC vs University of British Columbia) refer to the same entity\n",
    "- Dependency parsing\n",
    "    - Representing grammatical structure of a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1276e05c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Extracting named-entities using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f8113d84",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    University of British Columbia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is located in the beautiful province of \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    British Columbia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named entities:\n",
      " [('University of British Columbia', 'ORG'), ('British Columbia', 'GPE')]\n",
      "\n",
      "ORG means:  Companies, agencies, institutions, etc.\n",
      "GPE means:  Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(\n",
    "    \"University of British Columbia \"\n",
    "    \"is located in the beautiful \"\n",
    "    \"province of British Columbia.\"\n",
    ")\n",
    "displacy.render(doc, style=\"ent\")\n",
    "# Text and label of named entity span\n",
    "print(\"Named entities:\\n\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "print(\"\\nORG means: \", spacy.explain(\"ORG\"))\n",
    "print(\"GPE means: \", spacy.explain(\"GPE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15796732",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Dependency parsing using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "74325837",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"13c094df6a214ad2a13500ce8ab8c589-0\" class=\"displacy\" width=\"575\" height=\"224.5\" direction=\"ltr\" style=\"max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">like</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">cats</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-13c094df6a214ad2a13500ce8ab8c589-0-0\" stroke-width=\"2px\" d=\"M70,89.5 C70,2.0 225.0,2.0 225.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-13c094df6a214ad2a13500ce8ab8c589-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,91.5 L62,79.5 78,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-13c094df6a214ad2a13500ce8ab8c589-0-1\" stroke-width=\"2px\" d=\"M245,89.5 C245,2.0 400.0,2.0 400.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-13c094df6a214ad2a13500ce8ab8c589-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M400.0,91.5 L408.0,79.5 392.0,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"I like cats\")\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7278bdd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Many other things possible\n",
    "\n",
    "- A powerful tool \n",
    "- You can build your own rule-based searches. \n",
    "- You can also access word vectors using spaCy with bigger models. (Currently we are using `en_core_web_md` model.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49083a7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Summary\n",
    "\n",
    "- NLP is a big and very active field. \n",
    "- We broadly explored three topics: \n",
    "    - Word embeddings using pretrained models \n",
    "    - Topic modeling \n",
    "    - Basic text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d512c63d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Here are some resources if you want to get into NLP.\n",
    "\n",
    "- Check out this [CPSC course on NLP](https://www.cs.ubc.ca/~vshwartz/courses/CPSC436N-22/index.html). \n",
    "- The first resource I would recommend is the following book by Jurafsky and Martin. It's very approachable and fun. And the current edition is available online. \n",
    "    - [Speech and Language Processing by Dan Jurafsky and James H. Martin](https://web.stanford.edu/~jurafsky/slp3/)\n",
    "- There is a course taught at Stanford called [\"From languages to Information\"](http://web.stanford.edu/class/cs124/) by one of the co-authors of the above book, and it might be a good introduction to NLP for you. Most of the [course material](https://www.youtube.com/playlist?list=PLoROMvodv4rOFZnDyrlW3-nI7tMLtmiJZ&disable_polymer=true) and [videos]() are available for free. \n",
    "- If you are into deep learning, you may refer to [this course](https://cs224d.stanford.edu/). Again, all lecture videos are available on youtube. \n",
    "- If you want to look at current advancements in the field, you'll find all NLP related publications [here](https://www.aclweb.org/anthology/). "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330]",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
