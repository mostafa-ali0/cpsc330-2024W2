{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lecture 9: Class demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(\"..\"), (\"..\"), \"code\"))\n",
    "\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML, display\n",
    "from plotting_functions import *\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import Image\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "DATA_DIR = os.path.join(os.path.abspath(\"..\"), (\"..\"), \"data/\")\n",
    "\n",
    "# Ignore future deprecation warnings from sklearn (using `os` instead of `warnings` also works in subprocesses)\n",
    "import os\n",
    "os.environ['PYTHONWARNINGS']='ignore::FutureWarning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Changing global matplotlib settings for confusion matrix.\n",
    "plt.rcParams[\"xtick.labelsize\"] = 10\n",
    "plt.rcParams[\"ytick.labelsize\"] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning workflow \n",
    "\n",
    "- Here is a typical workflow of a supervised machine learning systems. \n",
    "- So far, we have talked about data splitting, preprocessing, some EDA, model selection with hyperparameter optimization, and interpretation in the context of linear models.\n",
    "- In the next few lectures, we will talk about evaluation metrics and model selection in terms of evaluation metrics, feature engineering, feature selection, and model transparency and interpretation.  \n",
    "\n",
    "![](../../img/ml-workflow.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation metrics for binary classification: Motivation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dataset for demonstration \n",
    "\n",
    "- Let's classify fraudulent and non-fraudulent transactions using Kaggle's [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud) data set.\n",
    "    - Note, that the credit card fraud detection data set is very large (150mb!) so we've stored it using [GitLFS](https://josh-ops.com/posts/add-files-to-git-lfs/) in a personal repo. You should download the data locally to follow-along, and be sure not to commit large data files to your repository!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# This dataset will be loaded using a URL instead of a CSV file\n",
    "DATA_URL = \"https://github.com/firasm/bits/raw/refs/heads/master/creditcard.csv\"\n",
    "\n",
    "cc_df = pd.read_csv(DATA_URL, encoding=\"latin-1\")\n",
    "# Sorting columns so it is easier to read\n",
    "cc_df = cc_df[['Class', 'Time', 'Amount'] + cc_df.columns[cc_df.columns.str.startswith('V')].to_list()]\n",
    "\n",
    "train_df, test_df = train_test_split(cc_df, test_size=0.3, random_state=111)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Good size dataset \n",
    "- For confidentially reasons, it only provides transformed features with PCA, which is a popular dimensionality reduction technique. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- We do not have categorical features. All features are numeric. \n",
    "- We have to be careful about the `Time` and `Amount` features. \n",
    "- We could scale `Amount`. \n",
    "- Do we want to scale time?\n",
    "    - In this lecture, we'll just drop the Time feature. \n",
    "    - We'll learn about time series briefly later in the course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's separate `X` and `y` for train and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_train_big, y_train_big = train_df.drop(columns=[\"Class\", \"Time\"]), train_df[\"Class\"]\n",
    "X_test, y_test = test_df.drop(columns=[\"Class\", \"Time\"]), test_df[\"Class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- It's easier to demonstrate evaluation metrics using an explicit validation set instead of using cross-validation. \n",
    "- So let's create a validation set. \n",
    "- Our data is large enough so it shouldn't be a problem. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_big, y_train_big, test_size=0.3, random_state=123\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier()\n",
    "pd.DataFrame(cross_validate(dummy, X_train, y_train, return_train_score=True)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Observations \n",
    "\n",
    "- `DummyClassifier` is getting 0.998 cross-validation accuracy!! \n",
    "- Should we be happy with this accuracy and deploy this `DummyClassifier` model for fraud detection? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "What's the class distribution? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_df[\"Class\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We have class imbalance. \n",
    "- We have MANY non-fraud transactions and only a handful of fraud transactions. \n",
    "- So in the training set, `most_frequent` strategy is labeling 199,025 (99.83%) instances correctly and only 339 (0.17%) instances incorrectly. \n",
    "- Is this what we want? \n",
    "- The \"fraud\" class is the important class that we want to spot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's scale the features and try `LogisticRegression`.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "pd.DataFrame(cross_validate(pipe, X_train, y_train, return_train_score=True)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- We are getting a slightly better score with logistic regression.  \n",
    "- What score should be considered an acceptable score here? \n",
    "- Are we actually spotting any \"fraud\" transactions? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- `.score` by default returns accuracy which is \n",
    "$$\\frac{\\text{correct predictions}}{\\text{total examples}}$$\n",
    "- Is accuracy a good metric here? \n",
    "- Is there anything more informative than accuracy that we can use here? \n",
    "\n",
    "Let's dig a little deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One way to get a better understanding of the errors is by looking at \n",
    "- false positives (type I errors), where the model incorrectly spots examples as fraud\n",
    "- false negatives (type II errors), where it's missing to spot fraud examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay  \n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "cm = ConfusionMatrixDisplay.from_estimator(\n",
    "    pipe, X_valid, y_valid, values_format=\"d\", display_labels=[\"Non fraud\", \"Fraud\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which types of errors would be most critical for the bank to address?\n",
    "\n",
    "<br><br><br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "predictions = pipe.predict(X_valid)\n",
    "TN, FP, FN, TP = confusion_matrix(y_valid, predictions).ravel()\n",
    "plot_confusion_matrix_example(TN, FP, FN, TP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Perfect prediction has all values down the diagonal\n",
    "- Off diagonal entries can often tell us about what is being mis-predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is \"positive\" and \"negative\"?\n",
    "\n",
    "- Two kinds of binary classification problems \n",
    "    - Distinguishing between two classes\n",
    "    - Spotting a class (spot fraud transaction, spot spam, spot disease)\n",
    "- In case of spotting problems, the thing that we are interested in spotting is considered \"positive\". \n",
    "- Above we wanted to spot fraudulent transactions and so they are \"positive\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can get a numpy array of confusion matrix as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "predictions = pipe.predict(X_valid)\n",
    "TN, FP, FN, TP = confusion_matrix(y_valid, predictions).ravel()\n",
    "print(\"Confusion matrix for fraud data set\")\n",
    "print(cm.confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Confusion matrix with cross-validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- You can also calculate confusion matrix with cross-validation using the `cross_val_predict` method.  \n",
    "- But then you cannot plot it in a nice format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "confusion_matrix(y_train, cross_val_predict(pipe, X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Precision, recall, f1 score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- We have been using `.score` to assess our models, which returns accuracy by default. \n",
    "- Accuracy is misleading when we have class imbalance.\n",
    "- We need other metrics to assess our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We'll discuss three commonly used metrics which are based on confusion matrix: \n",
    "    - recall\n",
    "    - precision\n",
    "    - f1 score \n",
    "- Note that these metrics will only help us assess our model.  \n",
    "- Later we'll talk about a few ways to address class imbalance problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and recall: toy example\n",
    "- Imagine that your model has identified everything outside the circle as non-fraud and everything inside the circle as fraud. \n",
    "\n",
    "![](../../img/precision-recall.png)\n",
    "\n",
    "![](../../img/fraud-precision-recall.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pipe_lr = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "predictions = pipe_lr.predict(X_valid)\n",
    "TN, FP, FN, TP = confusion_matrix(y_valid, predictions).ravel()\n",
    "print(cm.confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision \n",
    "\n",
    "Among the positive examples you identified, how many were actually positive?\n",
    "\n",
    "$$ precision = \\frac{TP}{TP+FP}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cm = ConfusionMatrixDisplay.from_estimator(\n",
    "    pipe, X_valid, y_valid, values_format=\"d\", display_labels=[\"Non fraud\", \"fraud\"]\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TP = %0.4f, FP = %0.4f\" % (TP, FP))\n",
    "precision = TP / (TP + FP)\n",
    "print(\"Precision: %0.4f\" % (precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recall \n",
    "\n",
    "Among all positive examples, how many did you identify correctly?\n",
    "$$ recall = \\frac{TP}{TP+FN} = \\frac{TP}{\\#positives} $$\n",
    "\n",
    "- Also called as sensitivity, coverage, true positive rate (TPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    pipe, X_valid, y_valid, values_format=\"d\", display_labels=[\"Non fraud\", \"fraud\"]\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TP = %0.4f, FN = %0.4f\" % (TP, FN))\n",
    "recall = TP / (TP + FN)\n",
    "print(\"Recall: %0.4f\" % (recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### F1-score\n",
    "\n",
    "- F1-score combines precision and recall to give one score, which could be used in hyperparameter optimization, for instance. \n",
    "- F1-score is a harmonic mean of precision and recall. \n",
    "\n",
    "\n",
    "$$ f1 = 2 \\times \\frac{ precision \\times recall}{precision + recall}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"precision: %0.4f\" % (precision))\n",
    "print(\"recall: %0.4f\" % (recall))\n",
    "f1_score = (2 * precision * recall) / (precision + recall)\n",
    "print(\"f1: %0.4f\" % (f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at all metrics at once on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "## Calculate evaluation metrics by ourselves\n",
    "data = {\n",
    "    \"calculation\": [],\n",
    "    \"accuracy\": [],\n",
    "    \"error\": [],\n",
    "    \"precision\": [],\n",
    "    \"recall\": [],\n",
    "    \"f1 score\": [],\n",
    "}\n",
    "data[\"calculation\"].append(\"manual\")\n",
    "data[\"accuracy\"].append((TP + TN) / (TN + FP + FN + TP))\n",
    "data[\"error\"].append((FP + FN) / (TN + FP + FN + TP))\n",
    "data[\"precision\"].append(precision)  # TP / (TP + FP)\n",
    "data[\"recall\"].append(recall)  # TP / (TP + FN)\n",
    "data[\"f1 score\"].append(f1_score)  # (2 * precision * recall) / (precision + recall)\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- `scikit-learn` has functions for [these metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "data[\"accuracy\"].append(accuracy_score(y_valid, pipe_lr.predict(X_valid)))\n",
    "data[\"error\"].append(1 - accuracy_score(y_valid, pipe_lr.predict(X_valid)))\n",
    "data[\"precision\"].append(\n",
    "    precision_score(y_valid, pipe_lr.predict(X_valid), zero_division=1)\n",
    ")\n",
    "data[\"recall\"].append(recall_score(y_valid, pipe_lr.predict(X_valid)))\n",
    "data[\"f1 score\"].append(f1_score(y_valid, pipe_lr.predict(X_valid)))\n",
    "data[\"calculation\"].append(\"sklearn\")\n",
    "df = pd.DataFrame(data)\n",
    "df.set_index([\"calculation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores match. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is a convenient function called `classification_report` in `sklearn` which gives this info. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        y_valid, pipe_lr.predict(X_valid), target_names=[\"non-fraud\", \"fraud\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interim summary \n",
    "\n",
    "- Accuracy is misleading when you have class imbalance. \n",
    "- A confusion matrix provides a way to break down errors made by our model. \n",
    "- We looked at three metrics based on confusion matrix: \n",
    "    - precision, recall, f1-score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Note that what you consider \"positive\" (fraud in our case) is important when calculating precision, recall, and f1-score. \n",
    "- If you flip what is considered positive or negative, we'll end up with different TP, FP, TN, FN, and hence different precision, recall, and f1-scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evalution metrics overview  \n",
    "There is a lot of terminology here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](../../img/evaluation-metrics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross validation with different metrics\n",
    "\n",
    "- We can pass different evaluation metrics with `scoring` argument of `cross_validate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "scoring = [\n",
    "    \"accuracy\",\n",
    "    \"f1\",\n",
    "    \"recall\",\n",
    "    \"precision\",\n",
    "]  # scoring can be a string, a list, or a dictionary\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "scores = cross_validate(\n",
    "    pipe, X_train_big, y_train_big, return_train_score=True, scoring=scoring\n",
    ")\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- You can also create [your own scoring function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html) and pass it to `cross_validate`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholding \n",
    "\n",
    "- We have a logistic regression model for fraud detection that predicts a value between 0 and 1, representing the probability that a given interactions is fraud.\n",
    "- We use thresholding to get the binary prediction. \n",
    "- A typical threshold is 0.5.\n",
    "    - A prediction of 0.90 $\\rightarrow$ a high likelihood that the transaction is fraudulent and we predict **fraud**\n",
    "    - A prediction of 0.20 $\\rightarrow$ a low likelihood that the transaction is non-fraudulent and we predict **Non fraud**\n",
    "- **What happens if the predicted score is equal to the chosen threshold?**\n",
    "\n",
    "[https://developers.google.com/machine-learning/crash-course/classification/thresholding](https://developers.google.com/machine-learning/crash-course/classification/thresholding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe_lr.predict_proba(X_valid)[:, 1] >= 0.9\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        y_valid, y_pred, target_names=[\"non-fraud\", \"fraud\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe_lr.predict_proba(X_valid)[:, 1] >= 0.2\n",
    "print(\n",
    "    classification_report(\n",
    "        y_valid, y_pred, target_names=[\"non-fraud\", \"fraud\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Precision-recall curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Confusion matrix provides a detailed break down of the errors made by the model. \n",
    "- But when creating a confusion matrix, we are using \"hard\" predictions. \n",
    "- Most classifiers in `scikit-learn` provide `predict_proba` method (or `decision_function`) which provides degree of certainty about predictions by the classifier. \n",
    "- Can we explore the degree of uncertainty to understand and improve the model performance? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Suppose for your business it is more costly to miss fraudulent transactions and suppose you want to achieve a recall of at least 75% for the \"fraud\" class. \n",
    "- One way to do this is by changing the threshold of `predict_proba`.\n",
    "    - `predict` returns 1 when `predict_proba`'s probabilities are above 0.5 for the \"fraud\" class.\n",
    "\n",
    "**Key idea: what if we threshold the probability at a smaller value so that we identify more examples as \"fraud\" examples?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's lower the threshold to 0.1. In other words, predict the examples as \"fraud\" if `predict_proba` > 0.1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe_lr.predict_proba(X_valid)[:, 1] > 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Operating point \n",
    "\n",
    "- Now our recall for \"fraud\" class is >= 0.75. \n",
    "- Setting a requirement on a classifier (e.g., recall of >= 0.75) is called setting the **operating point**. \n",
    "- It's usually driven by business goals and is useful to make performance guarantees to customers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision/Recall tradeoff \n",
    "\n",
    "- But there is a trade-off between precision and recall. \n",
    "- If you identify more things as \"fraud\", recall is going to increase but there are likely to be more false positives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's sweep through different thresholds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "thresholds = np.arange(0.0, 1.0, 0.1)\n",
    "thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to install `panel` package in order to run the code below locally. See the documentation [here](https://pyviz-dev.github.io/panel/getting_started/installation.html#jupyterlab-and-classic-notebook). \n",
    "\n",
    "```conda install -c pyviz panel```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "from panel import widgets\n",
    "from panel.interact import interact\n",
    "\n",
    "pn.extension()\n",
    "\n",
    "def f(threshold):\n",
    "    preds = pipe_lr.predict_proba(X_valid)[:, 1] > threshold\n",
    "    precision = np.round(precision_score(y_valid, preds), 4)\n",
    "    recall = np.round(recall_score(y_valid, preds), 4)\n",
    "    d = {'threshold':np.round(threshold, 4), 'Precision': precision, 'recall': recall}\n",
    "    return (d)\n",
    "\n",
    "interact(f, threshold=widgets.FloatSlider(start=0.0, end=0.99, step=0.05, value=0.5)).embed(max_opts=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decreasing the threshold\n",
    "\n",
    "- Decreasing the threshold means a lower bar for predicting fraud. \n",
    "    - You are willing to risk more false positives in exchange of more true positives. \n",
    "    - Recall would either stay the same or go up and precision is likely to go down\n",
    "    - Occasionally, precision may increase if all the new examples after decreasing the threshold are TPs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Increasing the threshold\n",
    "\n",
    "- Increasing the threshold means a higher bar for predicting fraud. \n",
    "    - Recall would go down or stay the same but precision is likely to go up \n",
    "    - Occasionally, precision may go down if TP decrease but FP do not decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision-recall curve\n",
    "\n",
    "Often, when developing a model, it's not always clear what the operating point will be and to understand the model better, it's informative to look at all possible thresholds and corresponding trade-offs of precision and recall in a plot.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(\n",
    "    y_valid, pipe_lr.predict_proba(X_valid)[:, 1]\n",
    ")\n",
    "plt.plot(recall, precision, label=\"logistic regression: PR curve\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.plot(\n",
    "    recall_score(y_valid, pipe_lr.predict(X_valid)),\n",
    "    precision_score(y_valid, pipe_lr.predict(X_valid)),    \n",
    "    \"or\",\n",
    "    markersize=10,\n",
    "    label=\"threshold 0.5\",\n",
    ")\n",
    "plt.legend(loc=\"best\", fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Each point in the curve corresponds to a possible threshold of the `predict_proba` output. \n",
    "- We can achieve a recall of 0.8 at a precision of 0.4. \n",
    "- The red dot marks the point corresponding to the threshold 0.5.\n",
    "- The top-right would be a perfect classifier (precision = recall = 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The threshold is not shown here, but it's going from 0 (upper-left) to 1 (lower right).\n",
    "- At a threshold of 0 (upper left), we are classifying everything  as \"fraud\".\n",
    "- Raising the threshold increases the precision but at the expense of lowering the recall. \n",
    "- At the extreme right, where the threshold is 1, we get into the situation where all the examples classified as \"fraud\" are actually \"fraud\"; we have no false positives. \n",
    "- Here we have a high precision but lower recall. \n",
    "- Usually the goal is to keep recall high as precision goes up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AP score \n",
    "\n",
    "- Often it's useful to have one number summarizing the PR plot (e.g., in hyperparameter optimization)\n",
    "- One way to do this is by computing the area under the PR curve. \n",
    "- This is called **average precision** (AP score)\n",
    "- AP score has a value between 0 (worst) and 1 (best). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "ap_lr = average_precision_score(y_valid, pipe_lr.predict_proba(X_valid)[:, 1])\n",
    "print(\"Average precision of logistic regression: {:.3f}\".format(ap_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the following handy function of `sklearn` to get the PR curve and the corresponding AP score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "PrecisionRecallDisplay.from_estimator(pipe_lr, X_valid, y_valid);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AP vs. F1-score\n",
    "\n",
    "It is very important to note this distinction:\n",
    "\n",
    "- F1 score is for a given threshold and measures the quality of `predict`.\n",
    "- AP score is a summary across thresholds and measures the quality of `predict_proba`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{important}\n",
    "Remember to pick the desired threshold based on the results on the validation set and **not** on the test set.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A few comments on PR curve\n",
    "\n",
    "- Different classifiers might work well in different parts of the curve, i.e., at different operating points.   \n",
    "- We can compare PR curves of different classifiers to understand these differences. \n",
    "- Let's create PR curves for SVC and Logistic Regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipe_svc = make_pipeline(StandardScaler(), SVC())\n",
    "pipe_svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n",
    "pipe_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to get precision and recall for different thresholds? \n",
    "- Use the function `precision_recall_curve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_lr, recall_lr, thresholds_lr = precision_recall_curve(\n",
    "    y_valid, pipe_lr.predict_proba(X_valid)[:, 1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Some more details\n",
    "\n",
    "- How are the thresholds and the precision and recall at the default threshold are calculated? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many thresholds? \n",
    "- It uses `n_thresholds` where `n_thresholds` is the number of unique `predict_proba` scores in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(pipe_lr.predict_proba(X_valid)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each threshold, precision and recall are calculated.  \n",
    "- The last precision and recall values are 1. and 0. respectively and do not have a corresponding threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_lr.shape, precision_lr.shape, recall_lr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "precision_lr, recall_lr, thresholds_lr = precision_recall_curve(\n",
    "    y_valid, pipe_lr.predict_proba(X_valid)[:, 1]\n",
    ")\n",
    "precision_svc, recall_svc, thresholds_svc = precision_recall_curve(\n",
    "    y_valid, pipe_svc.decision_function(X_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For logistic regression, what's the index of the threshold that is closest to the default threshold of 0.5? \n",
    "- We are subtracting 0.5 from the thresholds so that \n",
    "    - the numbers close to 0 become -0.5\n",
    "    - the numbers close to 1 become 0.5    \n",
    "    - the numbers close to 0.5 become 0\n",
    "- After this transformation, we are interested in the threshold index where the number is close to 0. So we take  absolute values and argmin.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_default_lr = np.argmin(np.abs(thresholds_lr - 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC doesn't have `predict_proba`. Instead it has something called `decision_function`. The index of the threshold that is closest to 0 of decision function is the default threshold in SVC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_default_lr = np.argmin(np.abs(thresholds_lr - 0.5))\n",
    "close_zero_svm = np.argmin(np.abs(thresholds_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PR curves for logistic regression and SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(recall_svc, precision_svc, label=\"svc\")\n",
    "plt.plot(recall_lr, precision_lr, label=\"logistic regression\")\n",
    "plt.plot(\n",
    "    recall_svc[close_zero_svm],\n",
    "    precision_svc[close_zero_svm],    \n",
    "    \"o\",\n",
    "    markersize=10,\n",
    "    label=\"default threshold svc\",\n",
    "    c=\"b\",\n",
    ")\n",
    "plt.plot(\n",
    "    recall_lr[close_default_lr],\n",
    "    precision_lr[close_default_lr],    \n",
    "    \"*\",\n",
    "    markersize=10,\n",
    "    label=\"default threshold logistic regression\",\n",
    "    c=\"r\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend(loc=\"best\", fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which model is doing better in this scenario: SVC or Logistic Regression? \n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "svc_preds = pipe_svc.predict(X_valid)\n",
    "lr_preds = pipe_lr.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(\"f1_score of logistic regression: {:.3f}\".format(f1_score(y_valid, lr_preds)))\n",
    "print(\"f1_score of svc: {:.3f}\".format(f1_score(y_valid, svc_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ap_lr = average_precision_score(y_valid, pipe_lr.predict_proba(X_valid)[:, 1])\n",
    "ap_svc = average_precision_score(y_valid, pipe_svc.decision_function(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Average precision of logistic regression: {:.3f}\".format(ap_lr))\n",
    "print(\"Average precision of SVC: {:.3f}\".format(ap_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Comparing the precision-recall curves provide us a detail insight compared to f1 score.\n",
    "- For example, F1 scores for SVC and logistic regressions are pretty similar. In fact, f1 score of logistic regression is a tiny bit better. \n",
    "- But when we look at the PR curve, we see that SVC is doing better than logistic regression for most of the other thresholds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Receiver Operating Characteristic (ROC) curve \n",
    "\n",
    "- Another commonly used tool to analyze the behavior of classifiers at different thresholds.  \n",
    "- Similar to PR curve, it considers all possible thresholds for a given classifier given by `predict_proba` but instead of precision and recall it plots false positive rate (FPR) and true positive rate (TPR or recall).\n",
    "$$ TPR = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "$$ FPR  = \\frac{FP}{FP + TN}$$\n",
    "\n",
    "- TPR $\\rightarrow$ Fraction of true positives out of all positive examples. \n",
    "- FPR $\\rightarrow$ Fraction of false positives out of all negative examples. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_valid, pipe_lr.predict_proba(X_valid)[:, 1])\n",
    "plt.plot(fpr, tpr, label=\"ROC Curve\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR (recall)\")\n",
    "\n",
    "default_threshold = np.argmin(np.abs(thresholds - 0.5))\n",
    "\n",
    "plt.plot(\n",
    "    fpr[default_threshold],\n",
    "    tpr[default_threshold],\n",
    "    \"or\",\n",
    "    markersize=10,\n",
    "    label=\"threshold 0.5\",\n",
    ")\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Different points on the ROC curve represent different classification thresholds. The curve starts at (0,0) and ends at (1, 1).\n",
    "    - (0, 0) represents the threshold that classifies everything as the negative class\n",
    "    - (1, 1) represents the threshold that classifies everything as the positive class \n",
    "- The ideal curve is close to the top left\n",
    "    - Ideally, you want a classifier with high recall while keeping low false positive rate.  \n",
    "- The red dot corresponds to the threshold of 0.5, which is used by predict.\n",
    "- We see that compared to the default threshold, we can achieve a better recall of around 0.8 without increasing FPR. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's compare ROC curve of different classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fpr_lr, tpr_lr, thresholds_lr = roc_curve(y_valid, pipe_lr.predict_proba(X_valid)[:, 1])\n",
    "\n",
    "fpr_svc, tpr_svc, thresholds_svc = roc_curve(\n",
    "    y_valid, pipe_svc.decision_function(X_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_default_lr = np.argmin(np.abs(thresholds_lr - 0.5))\n",
    "close_zero_svm = np.argmin(np.abs(thresholds_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(fpr_svc, tpr_svc, label=\"svc\")\n",
    "plt.plot(fpr_lr, tpr_lr, label=\"logistic regression\")\n",
    "plt.plot(\n",
    "    fpr_svc[close_zero_svm],\n",
    "    tpr_svc[close_zero_svm],\n",
    "    \"o\",\n",
    "    markersize=10,\n",
    "    label=\"default threshold svc\",\n",
    "    c=\"b\",\n",
    ")\n",
    "plt.plot(\n",
    "    fpr_lr[close_default_lr],\n",
    "    tpr_lr[close_default_lr],\n",
    "    \"*\",\n",
    "    markersize=10,\n",
    "    label=\"default threshold logistic regression\",\n",
    "    c=\"r\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate (Recall)\")\n",
    "plt.legend(loc=\"best\", fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Area under the curve (AUC)\n",
    "\n",
    "- AUC provides a single meaningful number for the model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_lr = roc_auc_score(y_valid, pipe_lr.predict_proba(X_valid)[:, 1])\n",
    "roc_svc = roc_auc_score(y_valid, pipe_svc.decision_function(X_valid))\n",
    "print(\"AUC for LR: {:.3f}\".format(roc_lr))\n",
    "print(\"AUC for SVC: {:.3f}\".format(roc_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- AUC of 0.5 means random chance. \n",
    "- AUC can be interpreted as evaluating the **ranking** of positive examples.\n",
    "- What's the probability that a randomly picked positive point has a higher score according to the classifier than a randomly picked point from the negative class. \n",
    "- AUC of 1.0 means all positive points have a higher score than all negative points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "```{important}\n",
    "For classification problems with imbalanced classes, using AP score or AUC is often much more meaningful than using accuracy. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to `PrecisionRecallCurveDisplay`, there is a `RocCurveDisplay` function in sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "RocCurveDisplay.from_estimator(pipe_lr, X_valid, y_valid);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's look at all the scores at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = [\"accuracy\", \"f1\", \"recall\", \"precision\", \"roc_auc\", \"average_precision\"]\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "scores = cross_validate(pipe, X_train_big, y_train_big, scoring=scoring)\n",
    "pd.DataFrame(scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "```{seealso}\n",
    "Check out [these visualization](https://github.com/dariyasydykova/open_projects/tree/master/ROC_animation) on ROC and AUC.  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{seealso}\n",
    "Check out how to plot ROC with cross-validation [here](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330]",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
