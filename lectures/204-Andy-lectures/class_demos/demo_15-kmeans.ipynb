{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4a5a1e4-cadb-4832-ae49-c4b519a86ce1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Lecture 15: Class demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86758193-4ebf-447f-8f5e-d7641f764f11",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Let's cluster images!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9d991b-c32c-44da-ac93-11238ce04d02",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "For this demo, I'm going to use two image datasets: \n",
    "1. A small subset of [200 Bird Species with 11,788 Images](https://www.kaggle.com/datasets/veeralakrishna/200-bird-species-with-11788-images) (available [here](../data/birds.zip))\n",
    "2. A tiny subset of [Food-101](https://www.kaggle.com/datasets/kmader/food41?select=food_c101_n10099_r32x32x1.h5)\n",
    "(available [here](../data/food.zip))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdeb43b-c67d-4a1f-bae0-64f8581104d9",
   "metadata": {},
   "source": [
    "To run the code below, you need to install pytorch and torchvision in the course conda environment. \n",
    "\n",
    "```conda install pytorch torchvision -c pytorch```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6b8cac-a1b4-4449-8c94-1bbd3cd1031d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "from torchvision.models import vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2653796-c2a4-4db1-9ee1-8451c9a18397",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224f0b6a-d7db-4db0-81b5-21320884df17",
   "metadata": {},
   "source": [
    "Let's start with  small subset of birds dataset. You can experiment with a bigger dataset if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfc808c-6616-4ace-8d90-76e863d09b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e47a41-a219-4a23-9b14-0a1af951cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0366dba9-0f2a-443d-9bcf-775afbff9f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e022916-14a1-401c-b6fd-76d2e001a141",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "IMAGE_SIZE = 200\n",
    "\n",
    "\n",
    "def read_img_dataset(data_dir):\n",
    "    data_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    image_dataset = datasets.ImageFolder(root=data_dir, transform=data_transforms)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        image_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0\n",
    "    )\n",
    "    dataset_size = len(image_dataset)\n",
    "    class_names = image_dataset.classes\n",
    "    inputs, classes = next(iter(dataloader))\n",
    "    return inputs, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5acd3a-c5b6-493a-ae4c-a03ad9a55c44",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_sample_imgs(inputs):\n",
    "    plt.figure(figsize=(10, 70))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Sample Training Images\")\n",
    "    plt.imshow(\n",
    "        np.transpose(utils.make_grid(inputs, padding=1, normalize=True), (1, 2, 0))\n",
    "    );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae48491a-27a8-44b6-9f3b-8a282939a084",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cluster_images(model, Z, inputs, cluster=0, img_shape=(3, 200, 200), n_img=5):\n",
    "    fig, axes = plt.subplots(\n",
    "        1,\n",
    "        n_img + 1,\n",
    "        subplot_kw={\"xticks\": (), \"yticks\": ()},\n",
    "        figsize=(10, 10),\n",
    "        gridspec_kw={\"hspace\": 0.3},\n",
    "    )\n",
    "    transpose_axes = (1, 2, 0)\n",
    "\n",
    "    if type(model).__name__ == \"KMeans\":\n",
    "        center = model.cluster_centers_[cluster]\n",
    "        dists = np.linalg.norm(Z - center, axis=1)\n",
    "        # mask = model.labels_ == cluster\n",
    "        # dists = np.sum((Z - center) ** 2, axis=1)\n",
    "        # dists[~mask] = np.inf\n",
    "        closest_index = np.argmin(dists)\n",
    "        inds = np.argsort(dists)[:n_img]\n",
    "        print(closest_index)\n",
    "        if Z.shape[1] == 1024:\n",
    "            axes[0].imshow(\n",
    "                np.transpose(\n",
    "                    inputs[closest_index].reshape(img_shape) / 2 + 0.5, transpose_axes\n",
    "                )\n",
    "            )\n",
    "            # axes[0].imshow(center.reshape((32,32)))\n",
    "        else:\n",
    "            axes[0].imshow(\n",
    "                np.transpose(center.reshape(img_shape) / 2 + 0.5, transpose_axes)\n",
    "            )\n",
    "        axes[0].set_title(\"Cluster center %d\" % (cluster))\n",
    "    if type(model).__name__ == \"GaussianMixture\":\n",
    "        center = model.means_[cluster]\n",
    "        cluster_probs = model.predict_proba(Z)[:, cluster]\n",
    "        inds = np.argsort(cluster_probs)[-n_img:]\n",
    "        dists = np.linalg.norm(Z - center, axis=1)\n",
    "        # Find the index of the closest feature vector to the mean\n",
    "        closest_index = np.argmin(dists)\n",
    "        if Z.shape[1] == 1024:\n",
    "            axes[0].imshow(\n",
    "                np.transpose(\n",
    "                    inputs[closest_index].reshape(img_shape) / 2 + 0.5, transpose_axes\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            axes[0].imshow(\n",
    "                np.transpose(center.reshape(img_shape) / 2 + 0.5, transpose_axes)\n",
    "            )\n",
    "        # axes[0].imshow(np.transpose(inputs[inds[0]].reshape(img_shape) / 2 + 0.5, transpose_axes))\n",
    "        axes[0].set_title(\"Cluster %d\" % (cluster))\n",
    "\n",
    "    i = 1\n",
    "    print(\"Image indices: \", inds)\n",
    "    for image in inputs[inds]:\n",
    "        axes[i].imshow(np.transpose(image / 2 + 0.5, transpose_axes))\n",
    "        i += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9284d9e8-8e63-4804-b767-09bd44bfeac9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path(\"../../data\")\n",
    "file_names = list(data_dir.joinpath(\"birds\").glob(\"*/*.jpg\"))\n",
    "n_images = len(file_names)\n",
    "BATCH_SIZE = n_images  # because our dataset is quite small\n",
    "birds_inputs, birds_classes = read_img_dataset(data_dir.joinpath(\"birds\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd0dc45-fe6c-47fa-b1d9-e1f64f492891",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_birds = birds_inputs.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fae593-21eb-4483-bd32-c09a679a12b5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_sample_imgs(birds_inputs[0:24, :, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb701e2-de98-485d-80d2-f0ab7ee35cfc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "For clustering we need to calculate distances between points. So we need a vector representation for each data point. A simplest way to create a vector representation of an image is by flattening the image.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d4f256-dda2-4dce-ab4e-7bae90745703",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "flatten_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "        transforms.Lambda(torch.flatten),\n",
    "    ]\n",
    ")\n",
    "flatten_images = datasets.ImageFolder(\n",
    "    root=data_dir.joinpath(\"birds\"), transform=flatten_transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e6d339-6225-4f10-858b-06191c9275aa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "flatten_dataloader = torch.utils.data.DataLoader(\n",
    "    flatten_images, batch_size=BATCH_SIZE, shuffle=True, num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cfd982-d309-495e-932d-c7bea40ad923",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "flatten_train, y_train = next(iter(flatten_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ada926-9142-4583-bd0f-f808e6e56654",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "flatten_images = flatten_train.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18551a5-ea53-473b-b634-cb14ce252998",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_shape = [3, 200, 200]\n",
    "img = flatten_images[20].reshape(image_shape)\n",
    "plt.imshow(np.transpose(img / 2 + 0.5, (1, 2, 0)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3231e93d-f886-4cd7-a070-bb10aca89c33",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "flatten_images.shape  # 200 by 200 images with 3 color channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f134de-f351-4f34-81d4-e9b756286bc0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 3\n",
    "km_flatten = KMeans(k, n_init=\"auto\", random_state=123)\n",
    "km_flatten.fit(flatten_images);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5436737f-9944-48c6-8ee5-aa0d0ab65876",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "km_flatten.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50070e2-b06e-493a-a479-46031993efeb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "flatten_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c11ebf3-96a6-43ee-a2fa-a2e6c69ec2f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "unflatten_inputs = np.array([img.reshape(image_shape) for img in flatten_images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf069885-73ee-4ba3-b63f-e2965ff53cf2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "flatten_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd9f3d9-53f4-40e6-babf-3d88935d7533",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for cluster in range(k):\n",
    "    get_cluster_images(km_flatten, flatten_images, unflatten_inputs, cluster, n_img=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881a5855-92dd-4b05-9b4b-14241e0fd648",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We see some mis-categorizations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a1580b-08ff-4ed9-b49e-269900fba3e3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "How about trying out a different input representation? Let's use transfer learning as a feature extractor with a pre-trained vision model. For each image in our dataset we'll pass it through a pretrained network and get a representation from the last layer, before the classification layer given by the pre-trained network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8ada4a-3f99-487b-9d58-d070c7dda274",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "![](../../img/cnn-ex.png)\n",
    "\n",
    "Source: https://cezannec.github.io/Convolutional_Neural_Networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee30b46d-f982-4003-bf24-0ff62de0b4d0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_features(model, inputs):\n",
    "    \"\"\"Extract output of densenet model\"\"\"\n",
    "    with torch.no_grad():  # turn off computational graph stuff\n",
    "        Z_train = torch.empty((0, 1024))  # Initialize empty tensors\n",
    "        y_train = torch.empty((0))\n",
    "        Z_train = torch.cat((Z_train, model(inputs)), dim=0)\n",
    "    return Z_train.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90125ae-aae3-4964-a2b7-684da02e7d35",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "densenet = models.densenet121(weights=\"DenseNet121_Weights.IMAGENET1K_V1\")\n",
    "densenet.classifier = torch.nn.Identity()  # remove that last \"classification\" layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a896a18e-a53f-4960-b0d9-c426eafabd5d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Z_birds = get_features(\n",
    "    densenet,\n",
    "    birds_inputs,\n",
    ").numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65afc017-4dcb-4fbe-b50d-c0703fc87045",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Z_birds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f8b896-0d73-4d67-85cc-ed402536c077",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Do we get better clustering with this representation? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f27d04d-fd22-41db-8a15-a8e8bf207eed",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 3\n",
    "km = KMeans(n_clusters=k, n_init=\"auto\", random_state=123)\n",
    "km.fit(Z_birds);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c84fc2e-04f5-4d08-a8ef-7923ff990296",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "km.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6a72f6-3429-47c3-b437-b0da7d064a38",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for cluster in range(k):\n",
    "    get_cluster_images(km, Z_birds, X_birds, cluster, n_img=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fac585-1965-4bc3-a0c8-4b59aa755c3f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "KMeans seems to be doing a good job. But cluster centers are not interpretable at all now.\n",
    "This dataset seems easier, as the birds have very distinct colors. Let's try a more complicated dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fea3c5-e3b0-4f96-a832-e0c0958dce49",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_names = list(data_dir.joinpath(\"food\").glob(\"*/*.jpg\"))\n",
    "n_images = len(file_names)\n",
    "BATCH_SIZE = n_images  # because our dataset is quite small\n",
    "food_inputs, food_classes = read_img_dataset(data_dir.joinpath(\"food\"))\n",
    "n_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaac060-721c-4710-8f4f-8aa631e8632c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_food = food_inputs.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b23869-fc39-4ad7-aa7e-429488c90694",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_sample_imgs(food_inputs[0:24, :, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d42021-74ca-4099-a2da-87acbd26129a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Z_food = get_features(\n",
    "    densenet,\n",
    "    food_inputs,\n",
    ").numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2283cd-c65e-4725-a541-0b4cc0ecc535",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Z_food.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1c75f4-92d8-482e-9786-3d4690a845b2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 5\n",
    "km = KMeans(n_clusters=k, n_init=\"auto\", random_state=123)\n",
    "km.fit(Z_food);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff5c067-0e76-4537-913c-5f5540be7eaf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "km.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af20731-ff3d-41bc-ab6a-b3aae7b5d2c0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for cluster in range(k):\n",
    "    get_cluster_images(km, Z_food, X_food, cluster, n_img=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d1d270-35e9-4186-8391-0fc0bacdec20",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "There are some mis-classifications but overall it seems pretty good! You can experiment with \n",
    "- Different values for number of clusters\n",
    "- Different pre-trained models\n",
    "- Other possible representations \n",
    "- Different image datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330]",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
