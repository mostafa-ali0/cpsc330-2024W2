{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f79fa3c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import (\n",
    "    ColumnTransformer,\n",
    "    TransformedTargetRegressor,\n",
    "    make_column_transformer,\n",
    ")\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f955414",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce2ca13",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 10: Regression metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfaf1d5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning outcomes \n",
    "\n",
    "From this lecture, students are expected to be able to:\n",
    "\n",
    "- Carry out feature transformations on somewhat complicated dataset. \n",
    "- Visualize transformed features as a dataframe. \n",
    "- Use `Ridge` and `RidgeCV`.\n",
    "- Explain how `alpha` hyperparameter of `Ridge` relates to the fundamental tradeoff. \n",
    "- Explain the effect of `alpha` on the magnitude of the learned coefficients. \n",
    "- Examine coefficients of transformed features.  \n",
    "- Appropriately select a scoring metric given a regression problem.\n",
    "- Interpret and communicate the meanings of different scoring metrics on regression problems.\n",
    "    - MSE, RMSE, $R^2$, MAPE\n",
    "- Apply log-transform on the target values in a regression problem with `TransformedTargetRegressor`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa50e44b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More comments on tackling class imbalance\n",
    "\n",
    "- In lecture 9 we talked about a few rather simple approaches to deal with class imbalance. \n",
    "- If you have a problem such as fraud detection problem where you want to spot rare events, you can also think of this problem as anomaly detection problem and use different kinds of algorithms such as [isolation forests](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html).  \n",
    "- If you are interested in this area, it might be worth checking out this book on this topic.\n",
    "[Imbalanced Learning: Foundations, Algorithms, and Applications](https://www.amazon.com/dp/1118074629/ref=as_li_ss_tl?&linkCode=sl1&tag=inspiredalgor-20&linkId=615e87a9105582e292ad2b7e2c7ea339&language=en_US)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0749a640",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "In this lecture, we'll be using [Kaggle House Prices dataset](https://www.kaggle.com/c/home-data-for-ml-course/). As usual, to run this notebook you'll need to download the data. For this dataset, train and test have already been separated. We'll be working with the train portion in this lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ecef50",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/housing-kaggle/train.csv\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.10, random_state=123)\n",
    "train_df.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d853bd52",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- The supervised machine learning problem is predicting housing price given features associated with properties. \n",
    "- Here, the target is `SalePrice`, which is continuous. So it's a **regression problem** (as opposed to classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851e78ad",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Separate X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56631b53",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=[\"SalePrice\"])\n",
    "y_train = train_df[\"SalePrice\"]\n",
    "\n",
    "X_test = test_df.drop(columns=[\"SalePrice\"])\n",
    "y_test = test_df[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f60402",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178f883c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292617ec",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7991ca-15a2-4fa6-ac60-fc47aba7119e",
   "metadata": {},
   "source": [
    "> Note: The notes have some info on using `pandas_profiling`. I am going to skip it in the interest of time, but do go through the notes fully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eaae0e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Feature types \n",
    "\n",
    "- We have mixed feature types and a bunch of missing values. \n",
    "- Now, let's identify feature types and transformations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2d7a8f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "numeric_looking_columns = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "print(numeric_looking_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc70a580",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "train_df[\"MSSubClass\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5583d23",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "MSSubClass: Identifies the type of dwelling involved in the sale.\t\n",
    "\n",
    "        20\t1-STORY 1946 & NEWER ALL STYLES\n",
    "        30\t1-STORY 1945 & OLDER\n",
    "        40\t1-STORY W/FINISHED ATTIC ALL AGES\n",
    "        45\t1-1/2 STORY - UNFINISHED ALL AGES\n",
    "        50\t1-1/2 STORY FINISHED ALL AGES\n",
    "        60\t2-STORY 1946 & NEWER\n",
    "        70\t2-STORY 1945 & OLDER\n",
    "        75\t2-1/2 STORY ALL AGES\n",
    "        80\tSPLIT OR MULTI-LEVEL\n",
    "        85\tSPLIT FOYER\n",
    "        90\tDUPLEX - ALL STYLES AND AGES\n",
    "       120\t1-STORY PUD (Planned Unit Development) - 1946 & NEWER\n",
    "       150\t1-1/2 STORY PUD - ALL AGES\n",
    "       160\t2-STORY PUD - 1946 & NEWER\n",
    "       180\tPUD - MULTILEVEL - INCL SPLIT LEV/FOYER\n",
    "       190\t2 FAMILY CONVERSION - ALL STYLES AND AGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ab215a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "train_df[\"MoSold\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2f4284",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "drop_features = [\"Id\"]\n",
    "numeric_features = [\n",
    "    \"BedroomAbvGr\",\n",
    "    \"KitchenAbvGr\",\n",
    "    \"LotFrontage\",\n",
    "    \"LotArea\",\n",
    "    \"OverallQual\",\n",
    "    \"OverallCond\",\n",
    "    \"YearBuilt\",\n",
    "    \"YearRemodAdd\",\n",
    "    \"MasVnrArea\",\n",
    "    \"BsmtFinSF1\",\n",
    "    \"BsmtFinSF2\",\n",
    "    \"BsmtUnfSF\",\n",
    "    \"TotalBsmtSF\",\n",
    "    \"1stFlrSF\",\n",
    "    \"2ndFlrSF\",\n",
    "    \"LowQualFinSF\",\n",
    "    \"GrLivArea\",\n",
    "    \"BsmtFullBath\",\n",
    "    \"BsmtHalfBath\",\n",
    "    \"FullBath\",\n",
    "    \"HalfBath\",\n",
    "    \"TotRmsAbvGrd\",\n",
    "    \"Fireplaces\",\n",
    "    \"GarageYrBlt\",\n",
    "    \"GarageCars\",\n",
    "    \"GarageArea\",\n",
    "    \"WoodDeckSF\",\n",
    "    \"OpenPorchSF\",\n",
    "    \"EnclosedPorch\",\n",
    "    \"3SsnPorch\",\n",
    "    \"ScreenPorch\",\n",
    "    \"PoolArea\",\n",
    "    \"MiscVal\",\n",
    "    \"YrSold\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19326720",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "set(numeric_looking_columns) - set(numeric_features) - set(drop_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d157b9da",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We'll treat the above numeric-looking features as categorical features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aaae43",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- There are a bunch of ordinal features in this dataset. \n",
    "- Ordinal features with the same scale \n",
    "    - Poor (Po), Fair (Fa), Typical (TA), Good (Gd), Excellent (Ex)\n",
    "    - These we'll be calling `ordinal_features_reg`.\n",
    "- Ordinal features with different scales\n",
    "    - These we'll be calling `ordinal_features_oth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d82a4d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ordinal_features_reg = [\n",
    "    \"ExterQual\",\n",
    "    \"ExterCond\",\n",
    "    \"BsmtQual\",\n",
    "    \"BsmtCond\",\n",
    "    \"HeatingQC\",\n",
    "    \"KitchenQual\",\n",
    "    \"FireplaceQu\",\n",
    "    \"GarageQual\",\n",
    "    \"GarageCond\",\n",
    "    \"PoolQC\",\n",
    "]\n",
    "ordering = [\n",
    "    \"Po\",\n",
    "    \"Fa\",\n",
    "    \"TA\",\n",
    "    \"Gd\",\n",
    "    \"Ex\",\n",
    "]  # if N/A it will just impute something, per below\n",
    "ordering_ordinal_reg = [ordering] * len(ordinal_features_reg)\n",
    "ordering_ordinal_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903fd11f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ordinal_features_oth = [\n",
    "    \"BsmtExposure\",\n",
    "    \"BsmtFinType1\",\n",
    "    \"BsmtFinType2\",\n",
    "    \"Functional\",\n",
    "    \"Fence\",\n",
    "]\n",
    "ordering_ordinal_oth = [\n",
    "    [\"NA\", \"No\", \"Mn\", \"Av\", \"Gd\"],\n",
    "    [\"NA\", \"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n",
    "    [\"NA\", \"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n",
    "    [\"Sal\", \"Sev\", \"Maj2\", \"Maj1\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n",
    "    [\"NA\", \"MnWw\", \"GdWo\", \"MnPrv\", \"GdPrv\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d981d71c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "categorical_features = list(\n",
    "    set(X_train.columns)\n",
    "    - set(numeric_features)\n",
    "    - set(ordinal_features_reg)\n",
    "    - set(ordinal_features_oth)\n",
    "    - set(drop_features)\n",
    ")\n",
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4b902c-b828-4331-9726-8bda421a855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"HouseStyle\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02514c9c-8c4a-4085-8e63-a2c43b47aac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"MoSold\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8472889-7426-4f3a-ba57-097e8daf722c",
   "metadata": {},
   "source": [
    "Do you notice anything about the most and least popular months to sell a house?\n",
    "\n",
    "Could this suggest an alternative feature to create insted of month sold?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4ede8d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Applying feature transformations\n",
    "\n",
    "- Since we have mixed feature types, let's use `ColumnTransformer` to apply different transformations on different features types.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a58df91",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer, make_column_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e66a06",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "numeric_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    StandardScaler(),\n",
    ")\n",
    "\n",
    "ordinal_transformer_reg = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OrdinalEncoder(categories=ordering_ordinal_reg),\n",
    ")\n",
    "\n",
    "ordinal_transformer_oth = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OrdinalEncoder(categories=ordering_ordinal_oth),\n",
    ")\n",
    "\n",
    "categorical_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e144f46",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor = make_column_transformer(\n",
    "    (\"drop\", drop_features),\n",
    "    (numeric_transformer, numeric_features),\n",
    "    (ordinal_transformer_reg, ordinal_features_reg),\n",
    "    (ordinal_transformer_oth, ordinal_features_oth),\n",
    "    (categorical_transformer, categorical_features),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcc56fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Examining the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c75d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.fit(X_train)  # Calling fit to examine all the transformers.\n",
    "preprocessor.named_transformers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4996c72",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ohe_columns = list(\n",
    "    preprocessor.named_transformers_[\"pipeline-4\"]\n",
    "    .named_steps[\"onehotencoder\"]\n",
    "    .get_feature_names_out(categorical_features)\n",
    ")\n",
    "\n",
    "new_columns = (\n",
    "    numeric_features + ordinal_features_reg + ordinal_features_oth + ohe_columns\n",
    ")\n",
    "\n",
    "X_train_enc = pd.DataFrame(\n",
    "    preprocessor.transform(X_train), index=X_train.index, columns=new_columns\n",
    ")\n",
    "\n",
    "X_train_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45a2a2d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_train_enc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0e3b5f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bea26cc",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dummy = DummyRegressor()\n",
    "pd.DataFrame(cross_validate(dummy, X_train, y_train, cv=10, return_train_score=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c96438",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13ffbad",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "lr = make_pipeline(preprocessor, Ridge())\n",
    "lr.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0497ef1a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "lr_preds = lr.predict(X_test)\n",
    "lr_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f380f4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Smallest coefficient: \", lr.named_steps[\"ridge\"].coef_.min())\n",
    "print(\"Largest coefficient:\", lr.named_steps[\"ridge\"].coef_.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4410a8f5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "lr_pipe = make_pipeline(preprocessor, Ridge())\n",
    "pd.DataFrame(cross_validate(lr_pipe, X_train, y_train, cv=10, return_train_score=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5749743-e653-4e47-84f6-d2c8591fd232",
   "metadata": {},
   "source": [
    "Does anyone notice a strange fold?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5035eb47",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Tuning `alpha` hyperparameter of `Ridge`\n",
    "- Recall that `Ridge` has a hyperparameter `alpha` that controls the fundamental tradeoff.\n",
    "- This is like `C` in `LogisticRegression` but, annoyingly, `alpha` is the inverse of `C`.\n",
    "- That is, large `C` is like small `alpha` and vice versa.\n",
    "- Smaller `alpha`: lower training error (overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8c75a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {\"ridge__alpha\": 10.0 ** np.arange(-5, 5, 1)}\n",
    "\n",
    "pipe_ridge = make_pipeline(preprocessor, Ridge())\n",
    "\n",
    "search = GridSearchCV(pipe_ridge, param_grid, return_train_score=True, n_jobs=-1)\n",
    "search.fit(X_train, y_train)\n",
    "train_scores = search.cv_results_[\"mean_train_score\"]\n",
    "cv_scores = search.cv_results_[\"mean_test_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed32c773",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.semilogx(param_grid[\"ridge__alpha\"], train_scores.tolist(), label=\"train\")\n",
    "plt.semilogx(param_grid[\"ridge__alpha\"], cv_scores.tolist(), label=\"cv\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"score\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3969520c-8885-4e08-b24c-189ad4734011",
   "metadata": {},
   "source": [
    "Just by eye it looks the best value is around $10^2$.\n",
    "\n",
    "But let's check that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0925351c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "best_alpha = search.best_params_\n",
    "print(best_alpha)\n",
    "search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a54e285-9d20-4fa0-9131-0ff359305cb1",
   "metadata": {},
   "source": [
    "What do we expect to happen to coefficients if we use a big or small values of $\\alpha$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6687d1c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pipe_bigalpha = make_pipeline(preprocessor, Ridge(alpha=1000))\n",
    "pipe_bigalpha.fit(X_train, y_train)\n",
    "bigalpha_coeffs = pipe_bigalpha.named_steps[\"ridge\"].coef_\n",
    "pd.DataFrame(\n",
    "    data=bigalpha_coeffs, index=new_columns, columns=[\"Coefficients\"]\n",
    ").sort_values(by=\"Coefficients\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99006c9a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pipe_smallalpha = make_pipeline(preprocessor, Ridge(alpha=0.01))\n",
    "pipe_smallalpha.fit(X_train, y_train)\n",
    "smallalpha_coeffs = pipe_smallalpha.named_steps[\"ridge\"].coef_\n",
    "pd.DataFrame(\n",
    "    data=smallalpha_coeffs, index=new_columns, columns=[\"Coefficients\"]\n",
    ").sort_values(by=\"Coefficients\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fb4aa1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pipe_bestalpha = make_pipeline(\n",
    "    preprocessor, Ridge(alpha=search.best_params_[\"ridge__alpha\"])\n",
    ")\n",
    "pipe_bestalpha.fit(X_train, y_train)\n",
    "bestalpha_coeffs = pipe_bestalpha.named_steps[\"ridge\"].coef_\n",
    "pd.DataFrame(\n",
    "    data=bestalpha_coeffs, index=new_columns, columns=[\"Coefficients\"]\n",
    ").sort_values(by=\"Coefficients\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762065ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `RidgeCV`\n",
    "\n",
    "Because it's so common to want to tune `alpha` with `Ridge`, sklearn provides a class called `RidgeCV`, which automatically tunes `alpha` based on cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86353774-e692-42ed-acc0-56966dfa03ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manual search code\n",
    "# param_grid = {\"ridge__alpha\": 10.0 ** np.arange(-6, 6, 1)}\n",
    "# pipe_ridge = make_pipeline(preprocessor, Ridge())\n",
    "# search = GridSearchCV(pipe_ridge, param_grid, return_train_score=True, n_jobs=-1)\n",
    "# search.fit(X_train, y_train)\n",
    "# train_scores = search.cv_results_[\"mean_train_score\"]\n",
    "# cv_scores = search.cv_results_[\"mean_test_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8984c5",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Using RidgeCV\n",
    "alphas = 10.0 ** np.arange(-6, 6, 1)\n",
    "ridgecv_pipe = make_pipeline(preprocessor, RidgeCV(alphas=alphas, cv=10))\n",
    "ridgecv_pipe.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e779cfc1",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "best_alpha = ridgecv_pipe.named_steps[\"ridgecv\"].alpha_\n",
    "best_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4893144c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exploring the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adebbeed",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ridge_tuned = make_pipeline(preprocessor, Ridge(alpha=best_alpha))\n",
    "ridge_tuned.fit(X_train, y_train)\n",
    "ridge_preds = ridge_tuned.predict(X_test)\n",
    "ridge_preds[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c456b6c8-c34f-4fb2-9ece-6d2135055a71",
   "metadata": {},
   "source": [
    "The nice thing about linear models is that we can interpret how they make predictions by inspecting the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f6d1e4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    data={\"coefficients\": ridge_tuned.named_steps[\"ridge\"].coef_}, index=new_columns\n",
    ")\n",
    "df.sort_values(\"coefficients\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50721008",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression scoring functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b781f0b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- We aren't doing classification anymore, so we can't just check for equality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3e6364",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ridge_tuned.predict(X_train) == y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfeb95c-4d16-46ce-b4d4-a9e70b3395fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_tuned.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b787ae",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We need a score that reflects how right/wrong each prediction is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a00cad-aa04-4194-b8e1-6703eaabc8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0429347",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "A number of popular scoring functions for regression. We are going to look at some common metrics: \n",
    "\n",
    "- mean squared error (MSE)\n",
    "- $R^2$\n",
    "- root mean squared error (RMSE)\n",
    "- MAPE\n",
    "\n",
    "See [sklearn documentation](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics) for more details. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c07f7b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mean squared error (MSE)\n",
    "\n",
    "- A common metric is mean squared error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ead603",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "preds = ridge_tuned.predict(X_train)\n",
    "np.mean((y_train - preds) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c569b2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Perfect predictions would have MSE=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78c8a8f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(y_train, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8f99f9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- MSE looks huge and unreasonable. There is an error of ~\\$1 Billion!\n",
    "- Is this score good or bad?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981dafc0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Unlike classification, with regression **our target has units**. \n",
    "- The target is in dollars, the mean squared error is in $dollars^2$ \n",
    "- The score also depends on the scale of the targets. \n",
    "- If we were working in cents instead of dollars, our MSE would be $10,000 \\times (100^2$) higher!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00ad652",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Multiply prediction and targets by 100 to convert dollars -> cents\n",
    "np.mean((y_train * 100 - preds * 100) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ccd36c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Root mean squared error or RMSE\n",
    "\n",
    "- The MSE above is in $dollars^2$.\n",
    "- A more relatable metric would be the root mean squared error, or RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc59c07",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(y_train, ridge_tuned.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33e2dfb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(y_train, ridge_tuned.predict(X_train), alpha=0.3)\n",
    "grid = np.linspace(y_train.min(), y_train.max(), 1000)\n",
    "plt.plot(grid, grid, \"--k\")\n",
    "plt.xlabel(\"true price\")\n",
    "plt.ylabel(\"predicte price\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c1511e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Here we can see a few cases where our prediction is way off.\n",
    "- Is there something weird about those houses, perhaps? Outliers? \n",
    "- Under the line means we're under-predicting, over the line means we're over-predicting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6a3844",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $R^2$ (not in detail)\n",
    "\n",
    "A common score is the $R^2$\n",
    "\n",
    "- This is the score that `sklearn` uses by default when you call score()\n",
    "- You can [read about it](https://en.wikipedia.org/wiki/Coefficient_of_determination) if interested.\n",
    "- $R^2$ measures the proportion of variability in $y$ that can be explained using $X$. \n",
    "- Independent of the scale of $y$.\n",
    "  - The max is 1.\n",
    "  - Nice because you can get a rough intuition of performance between different problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde43c0b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$R^2(y, \\hat{y}) = 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y_i})^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b413a9f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- The denominator measures the total variance in $y$.  \n",
    "- The amount of variability that is left unexplained after performing regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2906da61",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Key points:\n",
    "- The maximum is 1 for perfect predictions\n",
    "- Negative values are very bad: \"worse than DummyRegressor\" (very bad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8733ab2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MAPE\n",
    "\n",
    "- We got an RMSE of ~$30,000 before. \n",
    "\n",
    "Question: Is an error of \\$30,000 acceptable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae9bb50",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For a house worth \\$600k, it seems reasonable! That's 5% error.\n",
    "- For a house worth \\$60k, that is terrible. It's 50% error.\n",
    "\n",
    "We have both of these cases in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dce5f09",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How about looking at percent error? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a880bcc",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pred_train = ridge_tuned.predict(X_train)\n",
    "percent_errors = (pred_train - y_train) / y_train * 100.0\n",
    "percent_errors[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e553046a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These are both positive (predict too high) and negative (predict too low)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62012972",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can look at the absolute percent error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863c9d65",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "np.abs(percent_errors)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496c90ed-4e37-407a-9b0a-0fae75f06114",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(percent_errors).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38db3633",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And, like MSE, we can take the average over examples. This is called mean absolute percent error (MAPE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e198bbf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "mean_absolute_percentage_error(y_train, pred_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aca5480",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Ok, this is quite interpretable.\n",
    "- On average, we have around 10% error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390b4621",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transforming the targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4d6b0c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- When you have prices or count data, the target values are skewed. \n",
    "- Let's look at our target column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edae28d5",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(y_train, bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47154a82",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Log transform\n",
    "\n",
    "- A common trick in such cases is applying a log transform on the target column to make it more normal and less skewed.  \n",
    "- That is, transform $y\\rightarrow \\log(y)$.\n",
    "- Linear regression will usually work better on something that looks more normal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6ee011",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(np.log10(y_train), bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc78e0bd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can incorporate this in our pipeline using `sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2ffac8",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import TransformedTargetRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa060fc",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ttr = TransformedTargetRegressor(\n",
    "    Ridge(alpha=best_alpha), func=np.log1p, inverse_func=np.expm1\n",
    ")  # transformer for log transforming the target\n",
    "ttr_pipe = make_pipeline(preprocessor, ttr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaacf4b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ttr_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff26f99",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ttr_pipe.fit(X_train, y_train)\n",
    "# y_train automatically transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc19e64",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ttr_pipe.predict(X_train)  # predictions automatically un-transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783c6525",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_test, ttr_pipe.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd7ff2a-6f38-478d-8233-57d94df7781c",
   "metadata": {},
   "source": [
    "> Note: This is different then other transformations we have seen which change the features. This one changes the **target**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35602a03",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Different scoring functions with `cross_validate`\n",
    "\n",
    "- Let's try using MSE instead of the default $R^2$ score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0976d6",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    cross_validate(\n",
    "        ridge_tuned,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        return_train_score=True,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4534cd56-da31-4584-bc3c-155a85ed36c9",
   "metadata": {},
   "source": [
    "### Using regression metrics with `scikit-learn`\n",
    "\n",
    "- In `sklearn`, you will notice that it has negative version of the metrics above (e.g., `neg_mean_squared_error`, `neg_root_mean_squared_error`). \n",
    "- The reason for this is that scores return a value to maximize, the higher the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa0fe40",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def my_mape(true, pred):\n",
    "    return np.mean(np.abs((pred - true) / true))\n",
    "\n",
    "\n",
    "# make a scorer function that we can pass into cross-validation\n",
    "mape_scorer = make_scorer(my_mape, greater_is_better=False)\n",
    "\n",
    "pd.DataFrame(\n",
    "    cross_validate(\n",
    "        ridge_tuned, X_train, y_train, return_train_score=True, scoring=mape_scorer\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ed3250",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If you are finding `greater_is_better=False` argument confusing, here is the documentation: \n",
    "\n",
    "> greater_is_better(bool), default=True\n",
    "Whether score_func is a score function (default), meaning high is good, or a loss function, meaning low is good. In the latter case, the scorer object will sign-flip the outcome of the score_func.\n",
    "\n",
    "Since our custom scorer `mape` gives an error and not a score, I'm passing `False` to it and it'll sign flip so that we can interpret bigger numbers as better performance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aab6d04",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "scoring = {\n",
    "    \"r2\": \"r2\",\n",
    "    \"mape_scorer\": mape_scorer,  # just for demonstration for a custom scorer\n",
    "    \"sklearn MAPE\": \"neg_mean_absolute_percentage_error\",\n",
    "    \"neg_root_mean_square_error\": \"neg_root_mean_squared_error\",\n",
    "    \"neg_mean_squared_error\": \"neg_mean_squared_error\",\n",
    "}\n",
    "\n",
    "pd.DataFrame(\n",
    "    cross_validate(\n",
    "        ridge_tuned, X_train, y_train, return_train_score=True, scoring=scoring\n",
    "    )\n",
    ").T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924a635d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Are we getting the same `alpha` with mape? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78af1876",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {\"ridge__alpha\": 10.0 ** np.arange(-6, 6, 1)}\n",
    "pipe_ridge = make_pipeline(preprocessor, Ridge())\n",
    "search = GridSearchCV(\n",
    "    pipe_ridge, param_grid, return_train_score=True, n_jobs=-1, scoring=mape_scorer\n",
    ")\n",
    "search.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd38ea6-9c5b-422a-a347-cabc07c5d809",
   "metadata": {},
   "source": [
    "This is where using `greater_is_better=False` or the negative versions of the metrics becomes important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f977165d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best hyperparameter values: \", search.best_params_)\n",
    "print(\"Best score: %0.3f\" % (search.best_score_))\n",
    "pd.DataFrame(search.cv_results_)[\n",
    "    [\n",
    "        \"mean_train_score\",\n",
    "        \"mean_test_score\",\n",
    "        \"param_ridge__alpha\",\n",
    "        \"mean_fit_time\",\n",
    "        \"rank_test_score\",\n",
    "    ]\n",
    "].set_index(\"rank_test_score\").sort_index().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898c30fb-b7ff-448c-a731-628799565a56",
   "metadata": {},
   "source": [
    "Remember that the `best_score_` is going to be the negative of the MAPE here.\n",
    "\n",
    "So the best MAPE score is actually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daed7eda-21de-4ff0-b56f-c43e35eefc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best score: %0.3f\" % (-1 * search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8170c65b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using multiple metrics in `GridSearchCV` or `RandomizedSearchCV` \n",
    "\n",
    "- We could use multiple metrics with `GridSearchCV` or `RandomizedSearchCV`. \n",
    "- But if you do so, you need to set `refit` to the metric (string) for which the `best_params_` will be found and used to build the `best_estimator_` on the whole dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d2a283",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "search_multi = GridSearchCV(\n",
    "    pipe_ridge,\n",
    "    param_grid,\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1,\n",
    "    scoring=scoring,\n",
    "    refit=\"mape_scorer\",\n",
    ")\n",
    "search_multi.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a63820a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best hyperparameter values: \", search_multi.best_params_)\n",
    "print(\"Best score: %0.3f\" % (search_multi.best_score_))\n",
    "pd.DataFrame(search_multi.cv_results_).set_index(\"rank_test_mape_scorer\").sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eb7d46",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "search_multi.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d59b75",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What did we learn today? \n",
    "\n",
    "- House prices dataset target is price, which is numeric -> regression rather than classification\n",
    "- There are corresponding versions of all the tools we used:\n",
    "    - `DummyClassifier` -> `DummyRegressor`\n",
    "    - `LogisticRegression` -> `Ridge`\n",
    "- `Ridge` hyperparameter `alpha` is like `LogisticRegression` hyperparameter `C`, but opposite meaning\n",
    "- We'll avoid `LinearRegression` in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2748a2ec",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Scoring metrics\n",
    "- $R^2$ is the default .score(), it is unitless, 0 is bad, 1 is best\n",
    "- MSE (mean squared error) is in units of target squared, hard to interpret; 0 is best\n",
    "- RMSE (root mean squared error) is in the same units as the target; 0 is best\n",
    "- MAPE (mean absolute percent error) is unitless; 0 is best, 1 is bad"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330]",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
