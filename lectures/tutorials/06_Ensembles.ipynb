{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f4c84fd-271d-4826-b66c-1afe8c192a3e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](../img/330-banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdea385a-62f3-401d-91d0-3ab9e5191749",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Tutorial 6\n",
    "\n",
    "UBC 2024-25\n",
    "\n",
    "## Outline\n",
    "\n",
    "During this tutorial, you will work in groups to simulate the behaviour of averaging and stacking classifiers.\n",
    "\n",
    "All questions can be discussed with your classmates and the TAs - this is not a graded exercise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bec39ab-1444-4ec0-8131-76361e37583b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import string\n",
    "import sys\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(\"..\"), \"code\"))\n",
    "\n",
    "from plotting_functions import *\n",
    "from sklearn import datasets\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from utils import *\n",
    "DATA_DIR = os.path.join(os.path.abspath(\"..\"), \"data/\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14dd073-796e-4762-867b-03a8e1964e36",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "For this exercise, we will work with a new dataset on Heart Failure Prediction. You can download the dataset from [Kaggle](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction?resource=download). We also recommend taking a moment to read the Attribute Information included in this page, which will explain the features included in the dataset. The goal is to predict whether a patient is at risk of heart failure (class 1) or not (class 0).\n",
    "\n",
    "Use the cell below to read the dataset and check the first few rows (make sure the path matches the location on your computer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1bfdd5-edf9-4c18-a912-acbe860a3221",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "heart_df = pd.read_csv(DATA_DIR + \"heart.csv\")\n",
    "heart_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602aeb97-dc48-4ce7-ba70-76c429dacfcb",
   "metadata": {},
   "source": [
    "Luckily for us, it appears that the dataset is complete - we do not need to worry about imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaf27e9-9d06-4178-8df8-f3d2d9593f0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "heart_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe8f12d-cac4-43eb-b080-48a5eeae410e",
   "metadata": {},
   "source": [
    "The next few cells take care of the basic preprocessing steps needed before we get to the learning part, like creating a training/test split and creating a suitable `ColumnTransformer`. Run them before moving to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d106d619-3548-4270-8106-993db754e3d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(heart_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510dd658-92b7-488a-8642-2326141e58ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeric_features = [\"Age\", \"RestingBP\", \"Cholesterol\", \"MaxHR\", \"Oldpeak\"]\n",
    "\n",
    "categorical_features = [\n",
    "    \"ChestPainType\",\n",
    "    \"RestingECG\",\n",
    "    \"ST_Slope\",\n",
    "]\n",
    "\n",
    "binary_features = [\"Sex\", \"ExerciseAngina\"]\n",
    "passthrough_features = [\"FastingBS\"]\n",
    "target_column = \"HeartDisease\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b35c6c7-b5fd-4206-aa63-96e75fff60a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "binary_transformer = OneHotEncoder(drop=\"if_binary\", dtype=int)\n",
    "\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (numeric_transformer, numeric_features),\n",
    "    (binary_transformer, binary_features),\n",
    "    (categorical_transformer, categorical_features),\n",
    "    (\"passthrough\", passthrough_features),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9930aea1-92a9-4f5c-a47d-756bdf4152cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=[target_column])\n",
    "y_train = train_df[target_column]\n",
    "\n",
    "X_test = test_df.drop(columns=[target_column])\n",
    "y_test = test_df[target_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18a43d6-fec7-439a-894d-a59489ea5c81",
   "metadata": {},
   "source": [
    "The cell below shows that the dataset is balanced, which is good for our purposes. We will use accuracy as evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611a6fcb-5ca6-4dda-953e-9bac3de6da40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df[\"HeartDisease\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b7207a-72e0-4a50-a290-e5da54310dc5",
   "metadata": {},
   "source": [
    "## Averaging simulation\n",
    "\n",
    "In this portion of the exercise, you will need to split in 5 groups (groups can be of different size). Each group will then train a classifier to predict the target based on the available features. The classifiers to train are:\n",
    "\n",
    "- Decision Tree\n",
    "- kNN\n",
    "- Logistic regression\n",
    "- Random Forest\n",
    "- LightGBM \n",
    "\n",
    "For this exercise, we will not fine tune the classifiers and just use them \"off_shelf\". \n",
    "\n",
    "### <font color='red'>Question 1</font>\n",
    "\n",
    "After creating a pipeline with the preprocessor and your chosen classifier, use `cross_validate` to score it on the training set, and compare the results with the other groups. Which classifier has the best performance? Which show signs of overfitting? Which one is the slowest to train?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac390d2-b8e4-4e4d-98b6-b61691411cd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca8dbca-055c-44ec-8728-ad71097f29ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abd48fd2-09ff-4bc6-a0dc-dc77da5f10fd",
   "metadata": {},
   "source": [
    "### <font color='red'>Question 2</font>\n",
    "\n",
    "For this question, we will focus specifically on a small set of samples that were found to be more challenging to classify. You can get the samples by running the cell below.\n",
    "\n",
    "How many errors does your classifier make when classifying these samples? Compare your result with the other groups: which classifier does the fewest errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e42841-160e-4302-82eb-851c12521094",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "uncertain_indices = [122,  77,  49,  54,  12, 129,  35, 102,  39,  56]\n",
    "test_samples = test_df.iloc[uncertain_indices]\n",
    "test_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8354b67f-6448-4269-8c26-2c60b291a75a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "853b3823-2522-436e-bbda-25dbeab45789",
   "metadata": {},
   "source": [
    "### <font color='red'>Question 3</font>\n",
    "\n",
    "Now, you and the other groups are going to *average* your answers to see if your collective classification is better than the individual ones. Fill the table below with the answer from each classifier, and write down the final classification. Did the averaging classifier do better on these 10 samples than the individual ones?\n",
    "\n",
    "| Sample   | D.T.     | kNN.     | Log.reg. | R.F.     | LightGBM | Final prediction | Correct? |\n",
    "|----------|----------|----------|----------|----------|----------|----------|----------|\n",
    "| 122      |          |          |          |          |          |          |          |\n",
    "| 77       |          |          |          |          |          |          |          |\n",
    "| 49       |          |          |          |          |          |          |          |\n",
    "| 54       |          |          |          |          |          |          |          |\n",
    "| 12       |          |          |          |          |          |          |          |\n",
    "| 129      |          |          |          |          |          |          |          |\n",
    "| 35       |          |          |          |          |          |          |          |\n",
    "| 102      |          |          |          |          |          |          |          |\n",
    "| 39       |          |          |          |          |          |          |          |\n",
    "| 56       |          |          |          |          |          |          |          |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3924b853-a84a-4790-acf6-76ad7d82344b",
   "metadata": {},
   "source": [
    "Next, you may check if your answers match the ones of sklearn `VotingClassifier`, by running the cells below (for this to work, you will need to copy the classifiers from the other teams; also, change the names in the list if they are different). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16545aea-dae5-49a4-8f5b-d54786f8f039",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"logistic regression\": pipe_lr,\n",
    "    \"decision tree\": pipe_dt,\n",
    "    \"kNN\": pipe_kNN,\n",
    "    \"random forest\": pipe_rf,\n",
    "    \"LightGBM\": pipe_lgbm,\n",
    "}\n",
    "\n",
    "averaging_model = VotingClassifier(\n",
    "    list(classifiers.items()), voting=\"hard\"\n",
    ") \n",
    "\n",
    "averaging_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1b81e9-aefa-4c36-981d-dceb425b46eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "averaging_model.predict(X_test.iloc[uncertain_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e97e0-5a43-476d-b4d5-c0d4672973be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "averaging_model.score(X_test.iloc[uncertain_indices], y_test.iloc[uncertain_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3138578f-0f00-4f2d-8137-b30a76823da0",
   "metadata": {},
   "source": [
    "### <font color='red'>Question 4</font>\n",
    "\n",
    "If everything went according to plans, you should have gotten a better score on these 10 samples - hurray!\n",
    "\n",
    "But what about the overall classifier performance? Use cross validation to see if the `VotingClassifier` actually achieves a better validation accuracy than the other classifiers you and other groups have tried. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8830bad0-a206-4d7e-8b2e-99e4b6fa169e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores_averaging = cross_validate(averaging_model, X_train, y_train, return_train_score=True)\n",
    "pd.DataFrame(scores_averaging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a32ba7f-aacf-4510-a2fe-39460786d0e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(pd.DataFrame(scores_averaging).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dff7713-07f1-4a5c-aabd-2a579d3a8f2b",
   "metadata": {},
   "source": [
    "### <font color='red'>Question 5</font>\n",
    "\n",
    "To answer this question, repeat what you did in Question 3, but this time using **soft voting.** Complete the table with the predicted probability (for class 1) for each sample, and determine the final answer using their average.\n",
    "\n",
    "How is the performance of the averaging classifier with soft voting on the 10 uncertain samples?\n",
    "\n",
    "| Sample   | D.T.     | kNN.     | Log.reg. | R.F.     | LightGBM | Average  | Correct? |\n",
    "|----------|----------|----------|----------|----------|----------|----------|----------|\n",
    "| 122      |          |          |          |          |          |          |          |\n",
    "| 77       |          |          |          |          |          |          |          |\n",
    "| 49       |          |          |          |          |          |          |          |\n",
    "| 54       |          |          |          |          |          |          |          |\n",
    "| 12       |          |          |          |          |          |          |          |\n",
    "| 129      |          |          |          |          |          |          |          |\n",
    "| 35       |          |          |          |          |          |          |          |\n",
    "| 102      |          |          |          |          |          |          |          |\n",
    "| 39       |          |          |          |          |          |          |          |\n",
    "| 56       |          |          |          |          |          |          |          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d42f98d-7815-4436-bc03-efb77485c0f2",
   "metadata": {},
   "source": [
    "Once again, you can check if your answers match the ones of sklearn `VotingClassifier` with soft voting, by running the cells below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b273d8-e640-44b9-b4f5-75f047970668",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "averaging_model = VotingClassifier(\n",
    "    list(classifiers.items()), voting=\"soft\"\n",
    ") \n",
    "\n",
    "averaging_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a459cc-72ba-45fd-ae17-6abb68a303c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "averaging_model.predict(X_test.iloc[uncertain_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a3e814-26b0-49ac-a8ac-b4d69ee89c4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "averaging_model.score(X_test.iloc[uncertain_indices], y_test.iloc[uncertain_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cce0b7-1217-4318-be17-984c452da279",
   "metadata": {},
   "source": [
    "### <font color='red'>Question 6</font>\n",
    "\n",
    "Let's now use cross-validation to measure the overall performance of this classifier. How does it compare with the other options seen so far?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d5a5f3-dca7-4a7b-a8bd-b03dd740c607",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_averaging = cross_validate(averaging_model, X_train, y_train, return_train_score=True)\n",
    "pd.DataFrame(scores_averaging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4095648b-e0c5-4eb5-84c1-cb3d95e21fea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(pd.DataFrame(scores_averaging).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149a1773-6e01-4f37-b8f8-aaff7a9f203d",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "Stacking is another ensemble method that adds one more step to what we have seen the `VotingClassifier` do: instead of taking a majority vote or averaging predicted probability, it combines the output of different classifers to create a new feature vector for the sample.\n",
    "\n",
    "### <font color='red'>Question 7</font>\n",
    "\n",
    "How the new feature vectors are created depends on the parameters we use when creating the `StackingClassifier`. Review this using the related [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html), and answer the questions below.\n",
    "\n",
    "- What final estimator is used if none is specified as parameter?\n",
    "- What would the feature vector look like for sample 122 if stack_method = 'predict'? \n",
    "- What would the feature vector look like for sample 122 if stack_method = 'predict_proba'? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876117c8-b27b-4ad7-b0b5-aa20e843bcaa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1926c2d8-2f17-46d9-8093-098dca162156",
   "metadata": {},
   "source": [
    "### <font color='red'>Question 8</font>\n",
    "\n",
    "It is now time to try out the `StackingClassifier`. Run the cells below to create it and see how it performs on the uncertain samples and on the entire dataset. How does it compare to Averaging and the other classifiers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ea2055-fd03-4c23-a8c4-6eb40520f1dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stacking_model = StackingClassifier(list(classifiers.items()), stack_method = 'predict_proba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a5c720-3dca-470f-bb0b-37edcbee012b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stacking_model.fit(X_train, y_train)\n",
    "stacking_model.score(X_test.iloc[uncertain_indices], y_test.iloc[uncertain_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3690f06-2f9a-41aa-bec8-d10e6b99f130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores_stacking = cross_validate(stacking_model, X_train, y_train, return_train_score=True)\n",
    "pd.DataFrame(scores_stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3444dc-99ac-42e2-ad4d-92ee37dca9f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(pd.DataFrame(scores_stacking).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05162ed4-ded7-4a0f-bcf8-d9900a19451c",
   "metadata": {},
   "source": [
    "### <font color='red'>Question 9</font>\n",
    "\n",
    "An interesting thing about using a logistic regressor as final estimator is that we can observe the coefficients associated with each stacked classifier. These coefficients represent the confidence of the final estimator in each classifier's contribution. \n",
    "\n",
    "Check the coefficients by running the cells below. Which classifier is the most trustworthy? Which one is the least?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8190b391-d418-440e-a5cd-7513e37bb09e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    data=stacking_model.final_estimator_.coef_.flatten(),\n",
    "    index=classifiers.keys(),\n",
    "    columns=[\"Coefficient\"],\n",
    ").sort_values(\"Coefficient\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf4d6d0-4ccb-40ca-b828-2ec7b07e7078",
   "metadata": {},
   "source": [
    "### <font color='red'>Question 10</font>\n",
    "\n",
    "As last step, make the final call on which classifier, among all the ones you have seen today, should be used for this problem, and provide a thorough justification for your answer.\n",
    "\n",
    "Finally, do not forget to try your pick on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac29425-4ba5-4b43-b84e-b0d13921d6a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpsc330",
   "language": "python",
   "name": "cpsc330"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
